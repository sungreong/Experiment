{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd , sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import auc , roc_auc_score , confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder\n",
    "import re , os\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import *\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import scikitplot as skplt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.losses import Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/advice/Python/SR/Custom/\")\n",
    "from RAdam import RAdamOptimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = os.path.abspath(\"./Result\")\n",
    "ModelName = \"basic\"\n",
    "ResultPath = os.path.join(static , ModelName)    \n",
    "if tf.io.gfile.exists(ResultPath) :\n",
    "    tf.io.gfile.rmtree(ResultPath)\n",
    "    tf.io.gfile.makedirs(ResultPath)\n",
    "else :\n",
    "    tf.io.gfile.makedirs(ResultPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning:\n",
      "\n",
      "sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.integration import SkoptSampler\n",
    "import logging\n",
    "import plotly\n",
    "from optuna.logging import get_logger\n",
    "from optuna.structs import TrialState\n",
    "from optuna import type_checking\n",
    "from optuna.visualization.utils import _check_plotly_availability\n",
    "from optuna.visualization.utils import is_available\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb(split =None, _len_=None , n = None , ratio = None , key=None) :\n",
    "    print(f\"Category : {key}\")\n",
    "    split = tf.reshape(split , shape=(-1,))\n",
    "    split = tf.to_int32(split)\n",
    "    if _len_ < n :\n",
    "        first =  _len_\n",
    "        to = _len_\n",
    "        Cat = tf.one_hot(split ,depth=_len_)\n",
    "        print(f\"[{key}] Onehot Shape : [{first}]\")\n",
    "    else :\n",
    "        first =  _len_\n",
    "        to = int(_len_/2)\n",
    "        # 2/_len_\n",
    "        embeddings = tf.Variable(tf.truncated_normal([first , to], \n",
    "                                                     stddev =  0.1 ) ,\n",
    "                                 dtype = tf.float32,name=key)  \n",
    "        mod = sys.modules[__name__]\n",
    "        setattr(mod, f'embedding_{key}',embeddings)\n",
    "        Cat = tf.nn.embedding_lookup(embeddings, split)\n",
    "        Cat = tf.nn.dropout(Cat , ratio)\n",
    "        print(f\"[{key}] Onehot Shape : [{first}] --> Embedding Shape : [{to}] \")\n",
    "    return Cat\n",
    "\n",
    "# def EmbeddingLayer(X , objdict , inputratio ) :\n",
    "#     inputs = []\n",
    "#     for idx , (key, values )in enumerate(objdict.items()) :\n",
    "#         if idx == 0 :\n",
    "#             ## 1번째\n",
    "#             split = tf.slice(X , [0 , idx ] ,[-1 , key] ) # \n",
    "#             split = tf.nn.dropout(split , inputratio)\n",
    "#             inputs.append(split)\n",
    "#             category = tf.slice(X , [0 , key ] ,[-1 , 1 ] )\n",
    "#             _len_ = values\n",
    "#             Cat = emb(category , _len_ , 4 , inputratio)\n",
    "#             inputs.append(Cat)\n",
    "#         else :\n",
    "#             split = tf.slice(X , [0 , key ] ,[-1 , 1 ] )\n",
    "#             _len_ = values\n",
    "#             Cat = emb(split , _len_ , 4 , inputratio)\n",
    "#             inputs.append(Cat)\n",
    "#     concatenated_layer = tf.concat(inputs, axis=1, name='concatenate')\n",
    "#     return concatenated_layer\n",
    "\n",
    "def tf_feature(X , objcol , objdict , InputdropoutRate, totalcol) :\n",
    "    with tf.name_scope(f\"FeatureX\") :\n",
    "        if objcol == [] :\n",
    "            featureX = tf.nn.dropout(X , InputdropoutRate)\n",
    "        else :\n",
    "            featureX = EmbeddingLayer(X , objdict , InputdropoutRate, totalcol)\n",
    "    return featureX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_mish(x) :\n",
    "    return x * tf.nn.tanh(tf.nn.softplus(x))\n",
    "\n",
    "def get_weight_variable(shape, name=None,\n",
    "                        type='xavier_uniform', regularize=True, **kwargs):\n",
    "    initialise_from_constant = False\n",
    "    if type == 'xavier_uniform':\n",
    "        initial = xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "    elif type == 'xavier_normal':\n",
    "        initial = xavier_initializer(uniform=False, dtype=tf.float32)\n",
    "    elif type == 'he_normal':\n",
    "        initial = variance_scaling_initializer(uniform=False, factor=2.0, mode='FAN_IN', dtype=tf.float32)\n",
    "    elif type == 'he_uniform':\n",
    "        initial = variance_scaling_initializer(uniform=True, factor=2.0, mode='FAN_IN', dtype=tf.float32)\n",
    "    elif type == 'caffe_uniform':\n",
    "        initial = variance_scaling_initializer(uniform=True, factor=1.0, mode='FAN_IN', dtype=tf.float32)\n",
    "    elif type == 'simple':\n",
    "        stddev = kwargs.get('stddev', 0.02)\n",
    "        initial = tf.truncated_normal(shape, stddev=stddev, dtype=tf.float32)\n",
    "        initialise_from_constant = True\n",
    "    elif type == 'bilinear':\n",
    "        weights = _bilinear_upsample_weights(shape)\n",
    "        initial = tf.constant(weights, shape=shape, dtype=tf.float32)\n",
    "        initialise_from_constant = True\n",
    "    else:\n",
    "        raise ValueError('Unknown initialisation requested: %s' % type)\n",
    "\n",
    "    if name is None:  # This keeps to option open to use unnamed Variables\n",
    "        weight = tf.Variable(initial)\n",
    "    else:\n",
    "        if initialise_from_constant:\n",
    "            weight = tf.get_variable(name, initializer=initial)\n",
    "        else:\n",
    "            weight = tf.get_variable(name, shape=shape, initializer=initial)\n",
    "    if regularize:\n",
    "        tf.add_to_collection('weight_variables', weight)\n",
    "    return weight \n",
    "\n",
    "def Network(number , X , dims , dropoutRate , w_init, activation) :\n",
    "    with tf.variable_scope(f\"Network_{number}\", reuse = tf.AUTO_REUSE):\n",
    "        for idx , h_dim in enumerate(dims) :\n",
    "            if idx == 0 :    \n",
    "                TOTAL_DIM = X.get_shape().as_list()[1]\n",
    "                Weight =get_weight_variable(shape = [TOTAL_DIM , h_dim],\n",
    "                                            name=f\"Weight{idx}\",\n",
    "                                            type=w_init, regularize=True)\n",
    "                Bias = tf.get_variable(f\"Bias{idx}\",\n",
    "                                       shape = [h_dim] , dtype = tf.float32 , \n",
    "                                       initializer = tf.constant_initializer(0.0))\n",
    "                Layer = activation(tf.matmul( X , Weight) + Bias)\n",
    "                Layer = tf.contrib.nn.alpha_dropout(Layer , dropoutRate ) \n",
    "            else :\n",
    "                Weight =get_weight_variable(shape = [dims[idx-1] ,h_dim ], \n",
    "                                            name=f\"Weight{idx}\",\n",
    "                                            type=w_init, regularize=True)\n",
    "                Bias = tf.get_variable(f\"Bias{idx}\",\n",
    "                                       shape = [h_dim] , dtype = tf.float32 , \n",
    "                                       initializer = tf.constant_initializer(0.0))\n",
    "                Layer = tf.matmul( Layer , Weight) + Bias\n",
    "                if len(dims) == idx+1 :pass\n",
    "                else : \n",
    "                    Layer = activation(Layer)\n",
    "                    Layer = tf.contrib.nn.alpha_dropout(Layer , dropoutRate ) \n",
    "            tf.summary.histogram(f\"Weight{idx}\", Weight)\n",
    "            tf.summary.histogram(f\"Bias{idx}\", Bias)\n",
    "    return Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/advice/Python/SR/Data/income_evaluation.csv\")\n",
    "objcol = data.select_dtypes(\"object\").columns.tolist()\n",
    "data[objcol] = data[objcol].astype(\"category\")\n",
    "data.columns = [i.strip() for i in data.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"income\"] = data[\"income\"].cat.codes\n",
    "target = data.pop(\"income\")\n",
    "num_col = data.select_dtypes(\"int\").columns.tolist()\n",
    "cat_col = data.select_dtypes(\"category\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education-num     0\n",
       "marital-status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital-gain      0\n",
       "capital-loss      0\n",
       "hours-per-week    0\n",
       "native-country    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Train_X , Test_X , Train_y , Test_y =\\\n",
    "train_test_split(data , target , \n",
    "                 test_size = 0.3, \n",
    "                 stratify =target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>23040</td>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>51290</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1692</td>\n",
       "      <td>19</td>\n",
       "      <td>Private</td>\n",
       "      <td>375077</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20004</td>\n",
       "      <td>55</td>\n",
       "      <td>Private</td>\n",
       "      <td>194290</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26182</td>\n",
       "      <td>44</td>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>29591</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>2258</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5544</td>\n",
       "      <td>33</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>342458</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age     workclass  fnlwgt    education  education-num  \\\n",
       "23040   40       Private   51290      Masters             14   \n",
       "1692    19       Private  375077      HS-grad              9   \n",
       "20004   55       Private  194290      HS-grad              9   \n",
       "26182   44   Federal-gov   29591    Bachelors             13   \n",
       "5544    33     Local-gov  342458   Assoc-acdm             12   \n",
       "\n",
       "            marital-status        occupation    relationship    race    sex  \\\n",
       "23040   Married-civ-spouse    Prof-specialty         Husband   White   Male   \n",
       "1692         Never-married             Sales       Own-child   White   Male   \n",
       "20004             Divorced      Craft-repair   Not-in-family   White   Male   \n",
       "26182             Divorced      Tech-support   Not-in-family   White   Male   \n",
       "5544              Divorced   Protective-serv   Not-in-family   White   Male   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week  native-country  \n",
       "23040             0             0              50   United-States  \n",
       "1692              0             0              50   United-States  \n",
       "20004             0             0              40   United-States  \n",
       "26182             0          2258              40   United-States  \n",
       "5544              0             0              56   United-States  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass, 9==9==9?\n",
      "education, 16==16==16?\n",
      "marital-status, 7==7==7?\n",
      "occupation, 15==15==15?\n",
      "relationship, 6==6==6?\n",
      "race, 5==5==5?\n",
      "sex, 2==2==2?\n",
      "native-country, 42==41==41?\n",
      "===============\n",
      "Warning! Check : native-country\n",
      "Train Need Category : set()\n",
      "Test Need Category : {' Holand-Netherlands'}\n"
     ]
    }
   ],
   "source": [
    "for col in cat_col :\n",
    "    tr_n = set(Train_X[col].unique()) \n",
    "    te_n = set(Test_X[col].unique()) \n",
    "    intersect = tr_n & te_n\n",
    "    print(f\"{col}, {len(tr_n)}=={len(te_n)}=={len(intersect)}?\")\n",
    "    if len(tr_n) != len(te_n) :\n",
    "        te_need = tr_n.difference(te_n)\n",
    "        tr_need = te_n.difference(tr_n)\n",
    "        print(\"=\"*15)\n",
    "        print(f\"Warning! Check : {col}\")\n",
    "        print(f\"Train Need Category : {tr_need}\")\n",
    "        print(f\"Test Need Category : {te_need}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education\n",
      "marital-status\n",
      "occupation\n",
      "relationship\n",
      "race\n",
      "sex\n",
      "native-country\n"
     ]
    }
   ],
   "source": [
    "LabelEncoding = {}\n",
    "for col in cat_col :\n",
    "    print(col)\n",
    "    encoding = LabelEncoder()\n",
    "    category = list(set(list(Train_X[col].unique()) + [\"Unknown\"]))\n",
    "    encoding.fit(category)\n",
    "    Train_X[col] = Train_X[col].cat.add_categories('Unknown')\n",
    "    Train_X[col] = Train_X[col].fillna('Unknown')\n",
    "    Train_X[col] = encoding.transform(Train_X[col])\n",
    "    LabelEncoding[col] = encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "0.027339696884155273초\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "change = {}\n",
    "for col in cat_col :\n",
    "    change[col] = {}\n",
    "    LABLEncoder = LabelEncoding[col]\n",
    "    unknownlabel = set(Test_X[col].unique()).difference(set(LABLEncoder.classes_))\n",
    "    c = dict(zip(unknownlabel , [np.nan] * len(unknownlabel)))\n",
    "    print(c)\n",
    "    c[np.nan] = 'Unknown'\n",
    "    change[col] = c\n",
    "    \n",
    "print(\"{}초\".format(time()- start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education\n",
      "marital-status\n",
      "occupation\n",
      "relationship\n",
      "race\n",
      "sex\n",
      "native-country\n"
     ]
    }
   ],
   "source": [
    "for col in cat_col :\n",
    "    print(col)\n",
    "    LABLEncoder = LabelEncoding[col]\n",
    "    unknownlabel = set(Test_X[col].unique()).difference(set(LABLEncoder.classes_))\n",
    "    c = dict(zip(unknownlabel , [np.nan] * len(unknownlabel)))\n",
    "    c[np.nan] = 'Unknown'\n",
    "    Test_X[col] = Test_X[col].replace(c)   \n",
    "    Test_X[col] = LABLEncoder.transform(Test_X[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X = Train_X.astype(np.float32)\n",
    "Test_X = Test_X.astype(np.float32)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "numeric_features = num_col \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)])\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),])\n",
    "clf.fit(Train_X)\n",
    "Train_X[num_col] = clf.transform(Train_X)\n",
    "Test_X[num_col] = clf.transform(Test_X)\n",
    "#train_X[numcol] = imp_mean.transform(train_X[numcol])\n",
    "#test_X[numcol] = imp_mean.transform(test_X[numcol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 6, 7, 8, 9, 13]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalcol = data.columns.tolist()\n",
    "objinfo = [idx for idx , col in enumerate(totalcol) if col in cat_col ]\n",
    "objinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 10, 3: 17, 5: 8, 6: 16, 7: 7, 8: 6, 9: 3, 13: 43}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objdict = {}\n",
    "for i in objinfo :\n",
    "    column = totalcol[i]\n",
    "    objdict[i] = len(LabelEncoding[column].classes_)\n",
    "objdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dim = Train_X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholder:0' shape=(?, 14) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = tf.placeholder(tf.int64, name=\"Batchsize\")\n",
    "X =  tf.placeholder(tf.float32 , shape = [None , total_dim])\n",
    "y = tf.placeholder(tf.float32 , [None])\n",
    "dropoutRate = tf.placeholder(tf.float32, name =\"dropoutRate\")\n",
    "InputdropoutRate = tf.placeholder(tf.float32, name =\"InputdropoutRate\")\n",
    "#y = tf.string_to_number(y)\n",
    "X , y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'workclass',\n",
       " 'fnlwgt',\n",
       " 'education',\n",
       " 'education-num',\n",
       " 'marital-status',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'capital-gain',\n",
       " 'capital-loss',\n",
       " 'hours-per-week',\n",
       " 'native-country']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23040    4.0\n",
       "1692     4.0\n",
       "20004    4.0\n",
       "26182    1.0\n",
       "5544     2.0\n",
       "Name: workclass, dtype: float32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_X[cat_col[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ?',\n",
       " 1: ' Federal-gov',\n",
       " 2: ' Local-gov',\n",
       " 3: ' Never-worked',\n",
       " 4: ' Private',\n",
       " 5: ' Self-emp-inc',\n",
       " 6: ' Self-emp-not-inc',\n",
       " 7: ' State-gov',\n",
       " 8: ' Without-pay',\n",
       " 9: 'Unknown'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(np.arange(len(LabelEncoding[cat_col[0]].classes_)).tolist() , \n",
    "         LabelEncoding[cat_col[0]].classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def EmbeddingLayer(X , objdict , inputratio ,totalcol ) :\n",
    "    inputs = []\n",
    "    for idx , key in enumerate(totalcol) :\n",
    "        split = tf.slice(X , [0 , idx ] ,[-1 , 1 ] )\n",
    "        if idx in objdict :\n",
    "            category_n = objdict[idx]\n",
    "            Cat = emb(split , category_n , 4 , inputratio , key)\n",
    "            inputs.append(Cat)\n",
    "        else :\n",
    "            inputs.append(split)\n",
    "    concatenated_layer = tf.concat(inputs, axis=1, name='concatenate') \n",
    "    return concatenated_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category : workclass\n",
      "[workclass] Onehot Shape : [10] --> Embedding Shape : [5] \n",
      "Category : education\n",
      "[education] Onehot Shape : [17] --> Embedding Shape : [8] \n",
      "Category : marital-status\n",
      "[marital-status] Onehot Shape : [8] --> Embedding Shape : [4] \n",
      "Category : occupation\n",
      "[occupation] Onehot Shape : [16] --> Embedding Shape : [8] \n",
      "Category : relationship\n",
      "[relationship] Onehot Shape : [7] --> Embedding Shape : [3] \n",
      "Category : race\n",
      "[race] Onehot Shape : [6] --> Embedding Shape : [3] \n",
      "Category : sex\n",
      "[sex] Onehot Shape : [3]\n",
      "Category : native-country\n",
      "[native-country] Onehot Shape : [43] --> Embedding Shape : [21] \n"
     ]
    }
   ],
   "source": [
    "TransformX = tf_feature(X , cat_col , objdict , InputdropoutRate, totalcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(61)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransformX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dim = len(np.unique(Train_y))\n",
    "target_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit = Network(TransformX , \n",
    "#                 dims = [40 , 30, 20 , target_dim], \n",
    "#                 dropoutRate=dropoutRate)\n",
    "# Probs = tf.nn.softmax(Logit)\n",
    "# alpha = 0.95\n",
    "# y_one_hot = tf.add(alpha* tf.one_hot( tf.cast(y , tf.int32), \n",
    "#                                      depth=target_dim) ,\n",
    "#                    (1-alpha) / target_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.101882</td>\n",
       "      <td>-0.021566</td>\n",
       "      <td>0.086882</td>\n",
       "      <td>-0.086333</td>\n",
       "      <td>0.059974</td>\n",
       "      <td>-0.033789</td>\n",
       "      <td>-1.30757</td>\n",
       "      <td>0.043549</td>\n",
       "      <td>0.010537</td>\n",
       "      <td>-0.040405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141873</td>\n",
       "      <td>-0.122014</td>\n",
       "      <td>0.065916</td>\n",
       "      <td>-0.067428</td>\n",
       "      <td>-0.050964</td>\n",
       "      <td>-0.028894</td>\n",
       "      <td>0.049136</td>\n",
       "      <td>0.07292</td>\n",
       "      <td>-0.089792</td>\n",
       "      <td>0.05125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5        6   \\\n",
       "0  0.101882 -0.021566  0.086882 -0.086333  0.059974 -0.033789 -1.30757   \n",
       "\n",
       "         7         8         9   ...        51        52        53        54  \\\n",
       "0  0.043549  0.010537 -0.040405  ...  0.141873 -0.122014  0.065916 -0.067428   \n",
       "\n",
       "         55        56        57       58        59       60  \n",
       "0 -0.050964 -0.028894  0.049136  0.07292 -0.089792  0.05125  \n",
       "\n",
       "[1 rows x 61 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config=tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pd.DataFrame(sess.run(TransformX , \n",
    "                      feed_dict ={ X : Train_X.head(1).values ,\n",
    "                                  InputdropoutRate : 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(sess.run(Logit , \n",
    "#                       feed_dict ={ X : Train_X.head(1).values ,\n",
    "#                                   InputdropoutRate : 0.8 ,\n",
    "#                                   dropoutRate : 0.8}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# y_weight_info = compute_class_weight(\n",
    "#     class_weight= \"balanced\" , \n",
    "#     classes = np.unique(Train_y),\n",
    "#     y= np.squeeze(Train_y))\n",
    "\n",
    "# weight = tf.constant([ y_weight_info[1] ] ) #\n",
    "# print(f\"Weight : {y_weight_info[1]}\")\n",
    "# WCE = tf.nn.weighted_cross_entropy_with_logits(targets = y_one_hot ,\n",
    "#                                                logits = Logit , \n",
    "#                                                pos_weight =  weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss = tf.reduce_mean(WCE)\n",
    "# vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"Network\")\n",
    "# WEIGHTS = tf.get_collection(\"weight_variables\")\n",
    "# import re \n",
    "# L2 = []\n",
    "# for v in WEIGHTS :\n",
    "#     L2.append(tf.nn.l2_loss(v))\n",
    "# Loss += tf.add_n(L2)  * 0.0001\n",
    "# l1_regularizer = tf.contrib.layers.l1_regularizer(scale=0.005, scope=None)\n",
    "# regularization_penalty = tf.contrib.layers.apply_regularization(l1_regularizer, WEIGHTS )\n",
    "# Loss += regularization_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver = RAdamOptimizer(learning_rate= 2e-5, beta1 = 0.9 ,)\n",
    "# solver = solver.minimize(Loss ,var_list = vars )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _Loss_ = []\n",
    "# Epoch= 50000\n",
    "# mb_size = 500\n",
    "# print(\"Train\")\n",
    "# log = logging.getLogger('TF_log')\n",
    "# log.setLevel(logging.DEBUG)\n",
    "# fileHandler = logging.FileHandler(os.path.join(ResultPath,'tf_log.txt') , mode= \"w\")\n",
    "# log.addHandler(fileHandler)\n",
    "\n",
    "# config=tf.ConfigProto(log_device_placement=True)\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf.Session(config = config)\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# _Loss_ = []\n",
    "# _Epoch_ = []\n",
    "# _Epoch2_ = [0]\n",
    "# _trAUC_ , _teAUC_ = [0] , [0]\n",
    "\n",
    "\n",
    "# for epoch in range(Epoch) :\n",
    "#     idx = list(np.random.permutation(len(Train_X)))\n",
    "#     if (epoch > 0 ) & (epoch > 100) :\n",
    "#         idx = idx + BoostingList\n",
    "#         idx = list(np.random.permutation(idx))\n",
    "#         mb_size = 1000\n",
    "#     else : \n",
    "#         mb_size = 500\n",
    "#     XX = Train_X.iloc[idx, : ].values\n",
    "#     YY = Train_y[idx]\n",
    "#     batch_iter = int(len(XX) / mb_size)\n",
    "#     _Loss2_ = []\n",
    "#     for idx in range(batch_iter) :\n",
    "#         X_mb = XX[idx*mb_size:(idx+1)*mb_size]\n",
    "#         Y_mb = YY[idx*mb_size:(idx+1)*mb_size]\n",
    "#         Feed = {X : X_mb ,\n",
    "#                 y : Y_mb , \n",
    "#                 InputdropoutRate : 0.8 ,\n",
    "#                 dropoutRate : 0.8\n",
    "#                }\n",
    "#         _ , LOSS  = sess.run([solver , Loss] , feed_dict= Feed)\n",
    "#         _Loss2_.append(LOSS)\n",
    "#     _Loss_.append(np.mean(_Loss2_))\n",
    "#     _Epoch_.append(epoch)\n",
    "#     if (epoch > 0 ) & (epoch % 100 == 0) :\n",
    "#         Feed = { X : Train_X.values ,\n",
    "#                 InputdropoutRate : 0.8 ,\n",
    "#                 dropoutRate : 0.8\n",
    "#                }\n",
    "#         probs  = sess.run(Probs , feed_dict= Feed)\n",
    "#         trainDD = pd.DataFrame([Train_y ,probs[:,1]],\n",
    "#                           index = [\"t\",\"prob\"]).T\n",
    "#         ### Target 1 Boosting List\n",
    "#         DD2 = trainDD[(trainDD.t == 1) & (trainDD.prob <0.5)]\n",
    "#         BoostingList = 1 * DD2.index.tolist()\n",
    "#         msg = \"Epoch : {}  Boosting List : {}\".\\\n",
    "#               format(epoch,len(DD2.index.tolist()))\n",
    "#         log.info(msg)\n",
    "#     if (epoch > 0)  & (epoch % 100 == 0) :\n",
    "#         Feed = { X : Train_X.values ,\n",
    "#                 InputdropoutRate : 0.8 ,\n",
    "#                 dropoutRate : 0.8\n",
    "#                }\n",
    "#         clear_output()\n",
    "#         tr_real_target = np.squeeze(Train_y.values)\n",
    "#         tr_probs  = sess.run(Probs , feed_dict= Feed)\n",
    "#         tr_AUC = roc_auc_score(tr_real_target ,  tr_probs[:,1])\n",
    "#         trainDD = pd.DataFrame([Train_y.values ,probs[:,1]],\n",
    "#                           index = [\"t\",\"prob\"]).T\n",
    "#         ####################################################\n",
    "#         Feed = {X : Test_X.values  ,\n",
    "#                 InputdropoutRate : 0.8 ,\n",
    "#                 dropoutRate : 0.8\n",
    "#            }\n",
    "#         te_probs  = sess.run(Probs , feed_dict= Feed)\n",
    "#         te_real_target = np.squeeze(Test_y.values)\n",
    "#         te_AUC = roc_auc_score(te_real_target , te_probs[:,1])\n",
    "#         testDD = pd.DataFrame([te_real_target ,te_probs[:,1]],\n",
    "#                           index = [\"t\",\"prob\"]).T\n",
    "        \n",
    "#         fig , axes = plt.subplots(nrows=4 ,ncols=2,\n",
    "#                   figsize=(15,12) )\n",
    "#         plt.subplots_adjust(left=0.05, bottom=0.1, right=0.99, \n",
    "#                             top=0.95, wspace=None, hspace=0.5)\n",
    "#         ax = axes.flatten()\n",
    "#         sns.boxplot(x=\"t\", y=\"prob\", data=trainDD, ax = ax[0])\n",
    "#         ax[0].set_title(\"train : {:.3f}\".format(tr_AUC), fontsize= 25)\n",
    "#         sns.boxplot(x=\"t\", y=\"prob\", data=testDD, ax = ax[1])\n",
    "#         ax[1].set_title(\"test : {:.3f}\".format(te_AUC), fontsize= 25)\n",
    "#         skplt.metrics.plot_ks_statistic(Train_y.values, tr_probs , \n",
    "#                                         ax = ax[2] , \n",
    "#                                         title = \"[Train] KS Static PLOT\")\n",
    "#         skplt.metrics.plot_ks_statistic(Test_y.values, te_probs ,\n",
    "#                                         ax = ax[3], \n",
    "#                                         title = \"[Test] KS Static PLOT\")\n",
    "\n",
    "#         ax3 = plt.subplot(413)\n",
    "#         plt.subplots_adjust(left=0.05, bottom=0.1, right=0.99, \n",
    "#                             top=0.95, wspace=None, hspace=0.7)\n",
    "#         ax3.plot(_Epoch_ , _Loss_ )\n",
    "#         ax3.set_title(msg, fontsize= 20)\n",
    "#         _Epoch2_.append(epoch)\n",
    "#         _trAUC_.append(tr_AUC)\n",
    "#         _teAUC_.append(te_AUC)\n",
    "#         ax4 = plt.subplot(414)\n",
    "#         ax4.plot(_Epoch2_ , _trAUC_ , label = \"train auc\")\n",
    "#         ax4.plot(_Epoch2_ , _teAUC_ , label = \"test auc\")\n",
    "#         ax4.legend()\n",
    "#         msg = \"Epoch : {} / AUC | Train : {:.2f} Test : {:.2f}\".format(epoch , \n",
    "#                                                                100*tr_AUC ,\n",
    "#                                                                100*te_AUC\n",
    "#                                                               )\n",
    "#         ax4.set_title(msg, fontsize= 20)\n",
    "#         savefig = os.path.join(ResultPath,f'plot.{epoch:04d}.png')\n",
    "#         plt.savefig(savefig)\n",
    "#         plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.1356384e-01,  4.0000000e+00, -2.5518733e-01, ...,\n",
       "        -2.1504386e-01,  3.6562645e-01,  3.9000000e+01],\n",
       "       [-1.2138724e+00,  4.0000000e+00,  1.6279891e+00, ...,\n",
       "        -2.1504386e-01, -3.6846243e-02,  3.9000000e+01],\n",
       "       [ 1.0188165e-01,  4.0000000e+00, -1.4701250e+00, ...,\n",
       "        -2.1504386e-01,  2.0463738e-01,  3.9000000e+01],\n",
       "       ...,\n",
       "       [-2.6360559e-01,  4.0000000e+00, -1.3950696e+00, ...,\n",
       "        -2.1504386e-01, -8.4179163e-01,  3.9000000e+01],\n",
       "       [ 1.1983434e+00,  4.0000000e+00, -1.2089788e+00, ...,\n",
       "        -2.1504386e-01,  7.6809919e-01,  3.9000000e+01],\n",
       "       [ 5.4046637e-01,  4.0000000e+00, -5.9024364e-01, ...,\n",
       "        -2.1504386e-01, -1.9783533e-01,  3.9000000e+01]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(trial = None) :\n",
    "    nodes_1 = trial.suggest_int(\"nodes_1\",30,65)\n",
    "    nodes_2 = trial.suggest_int(\"nodes_2\",30,65)\n",
    "    nodes_3 = trial.suggest_int(\"nodes_3\",10,30)\n",
    "    \n",
    "    init_candidate =\\\n",
    "    [\"xavier_uniform\",\"xavier_normal\", \n",
    "     \"he_normal\", \"he_uniform\",\"caffe_uniform\"]\n",
    "    activate_candidate = \\\n",
    "    [tf.nn.selu, tf_mish , tf.nn.leaky_relu , tf.nn.elu , tf.nn.relu ]\n",
    "    activate_int = [0,1,2,3,4]\n",
    "    \n",
    "    target_n = 2 \n",
    "    init =\\\n",
    "    trial.suggest_categorical('weight_init', init_candidate)\n",
    "    selected =\\\n",
    "    trial.suggest_categorical('activation', activate_int)\n",
    "    activation = activate_candidate[selected]\n",
    "    alpha = trial.suggest_uniform('smoothLabel', 0.7, 0.99)\n",
    "    ##################################################################\n",
    "    tf.reset_default_graph()\n",
    "    batch_size = tf.placeholder(tf.int64, name=\"Batchsize\")\n",
    "    X =  tf.placeholder(tf.float32 , shape = [None , total_dim])\n",
    "    y = tf.placeholder(tf.float32 , [None])\n",
    "    regularizer = tf.placeholder(tf.float32, name =\"regularizer\")\n",
    "    dropoutRate = tf.placeholder(tf.float32, name =\"dropoutRate\")\n",
    "    InputdropoutRate = tf.placeholder(tf.float32, name =\"InputdropoutRate\")\n",
    "    ##################################################################\n",
    "    TransformX = tf_feature(X , cat_col , objdict , InputdropoutRate, totalcol)\n",
    "    dims = [nodes_1 , nodes_2, nodes_3 , target_dim]\n",
    "    print(dims)\n",
    "    Logit = Network(number= trial.number ,\n",
    "                    X = TransformX , \n",
    "                    dims = dims , \n",
    "                    dropoutRate=dropoutRate,\n",
    "                    w_init = init,\n",
    "                    activation = activation)\n",
    "    Probs = tf.nn.softmax(Logit)\n",
    "    y_one_hot = tf.one_hot( tf.cast(y , tf.int32), depth=target_dim) \n",
    "#     y_one_hot = tf.add(alpha* yy,(1-alpha) / target_dim)\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    y_weight_info = compute_class_weight(\n",
    "        class_weight= \"balanced\" , \n",
    "        classes = np.unique(Train_y),\n",
    "        y= np.squeeze(Train_y))\n",
    "\n",
    "    weight = tf.constant([ y_weight_info[1] ] ) #\n",
    "#     WCE = tf.nn.weighted_cross_entropy_with_logits(targets = y_one_hot ,\n",
    "#                                                    logits = Logit , \n",
    "#                                                    pos_weight =  weight)\n",
    "    WCE = tf.losses.softmax_cross_entropy(onehot_labels= y_one_hot , \n",
    "                                          logits= Logit , \n",
    "                                          label_smoothing= alpha ,\n",
    "                                          weights= weight,\n",
    "                                          reduction= Reduction.MEAN\n",
    "                                         )\n",
    "    \n",
    "    Loss = tf.reduce_mean(WCE)\n",
    "    vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                             scope=f\"Network_{trial.number}\")\n",
    "    WEIGHTS = tf.get_collection(\"weight_variables\")\n",
    "    L2 = []\n",
    "    for v in WEIGHTS :\n",
    "        L2.append(tf.nn.l2_loss(v))\n",
    "    Loss += tf.add_n(L2)  * regularizer\n",
    "    l1_regularizer = tf.contrib.layers.l1_regularizer(scale=0.005, scope=None)\n",
    "    regularization_penalty = tf.contrib.layers.apply_regularization(l1_regularizer, WEIGHTS )\n",
    "    Loss += regularizer * regularization_penalty\n",
    "    kwargs = {}\n",
    "    kwargs['learning_rate'] = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    optimizer = RAdamOptimizer(**kwargs)\n",
    "    solver = optimizer.minimize(Loss ,var_list = vars )\n",
    "\n",
    "    prediction = tf.argmax(Probs, 1)\n",
    "    correct = tf.argmax(y_one_hot, 1)\n",
    "    equality = tf.equal(prediction, correct)\n",
    "    accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "    tf.summary.scalar(f\"Accuracy_{trial.number}\",accuracy)\n",
    "    tf.summary.scalar(f\"Loss_{trial.number}\",Loss)\n",
    "    tf.summary.histogram(f\"Probability_{trial.number}\", Probs)\n",
    "    merged = tf.summary.merge_all()\n",
    "    return [Probs , Loss , solver, \n",
    "            X , y , dropoutRate , InputdropoutRate ,regularizer,\n",
    "            merged]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = 0.0\n",
    "log = logging.getLogger('optuna')\n",
    "log.setLevel(logging.DEBUG)\n",
    "fileHandler = logging.FileHandler(\n",
    "    os.path.join(ResultPath,'optuna_log.txt') , mode= \"w\")\n",
    "log.addHandler(fileHandler)\n",
    "\n",
    "\n",
    "def objective(trial):   \n",
    "    Lists = create_model(trial =trial)\n",
    "    (Probs , Loss , solver, \n",
    "     X , y , dropoutRate , InputdropoutRate , regularizer,\n",
    "     merged\n",
    "    ) = Lists\n",
    "    \n",
    "    if trial.number > 11 :\n",
    "        Epoch = 1000\n",
    "        mb_size = 1000\n",
    "    else :\n",
    "        Epoch = 500\n",
    "        mb_size = 500\n",
    "    config=tf.ConfigProto(log_device_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config = config)\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(ResultPath,\n",
    "                                                      f'train_{trial.number}'),\n",
    "                                         sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    config = projector.ProjectorConfig()\n",
    "    mod = sys.modules[__name__]\n",
    "    for cat in cat_col :\n",
    "        if len(LabelEncoding[cat].classes_) < 5 :\n",
    "            continue\n",
    "        index2word_map = dict(zip(np.arange(len(LabelEncoding[cat].classes_)).tolist() , \n",
    "                                 LabelEncoding[cat].classes_))\n",
    "        metadata_file = os.path.join(ResultPath, f'train_{trial.number}', \n",
    "                               f'metadata_{trial.number}_{cat}.tsv') \n",
    "        with open(metadata_file, \"w\") as metadata:\n",
    "            metadata.write('Name\\tClass\\n')\n",
    "            for k, v in index2word_map.items():\n",
    "                metadata.write('%s\\t%d\\n' % (v, k))\n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = getattr(mod,  f'embedding_{cat}').name\n",
    "        # Link this tensor to its metadata file (e.g. labels).\n",
    "        embedding.metadata_path = metadata_file\n",
    "    projector.visualize_embeddings(train_writer, config)\n",
    "\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    _Loss_ = []\n",
    "    _Epoch_ = []\n",
    "    _Epoch2_ = [0]\n",
    "    _trAUC_ , _teAUC_ = [0] , [0]\n",
    "    for epoch in range(Epoch) :\n",
    "        if epoch < 100 :\n",
    "            regularizer_rate = 1e-5\n",
    "        elif epoch < 200 :\n",
    "            regularizer_rate = 1e-4\n",
    "        elif epoch < 400 :\n",
    "            regularizer_rate = 1e-3\n",
    "        elif epoch < 800 :\n",
    "            regularizer_rate = 1e-2\n",
    "        idx = list(np.random.permutation(len(Train_X)))\n",
    "        XX = Train_X.iloc[idx, : ].values\n",
    "        YY = Train_y[idx]\n",
    "        batch_iter = int(len(XX) / mb_size)\n",
    "        _Loss2_ = []\n",
    "        for idx in range(batch_iter) :\n",
    "            X_mb = XX[idx*mb_size:(idx+1)*mb_size]\n",
    "            Y_mb = YY[idx*mb_size:(idx+1)*mb_size]\n",
    "            Feed = {X : X_mb ,\n",
    "                    y : Y_mb , \n",
    "                    regularizer : regularizer_rate,\n",
    "                    InputdropoutRate : 0.8 ,\n",
    "                    dropoutRate : 0.8\n",
    "                   }\n",
    "            _ , LOSS  = sess.run([solver , Loss] , feed_dict= Feed)\n",
    "            _Loss2_.append(LOSS)\n",
    "        _Loss_.append(np.mean(_Loss2_))\n",
    "        _Epoch_.append(epoch)\n",
    "        if epoch % 100 == 0 :\n",
    "            saver.save(sess, os.path.join(ResultPath, f'train_{trial.number}', \"model.ckpt\" ), epoch)\n",
    "            Feed = {X : Test_X.values  ,\n",
    "                    y : Test_y.values ,\n",
    "                    regularizer : regularizer_rate,\n",
    "                    InputdropoutRate : 0.8 ,\n",
    "                    dropoutRate : 0.8,\n",
    "                   }\n",
    "            summary = sess.run( merged ,feed_dict= Feed)\n",
    "            train_writer.add_summary(summary, epoch)\n",
    "            \n",
    "    train_writer.close()\n",
    "    Feed = {X : Test_X.values  ,\n",
    "                InputdropoutRate : 0.8 ,\n",
    "                dropoutRate : 0.8\n",
    "           }\n",
    "    te_probs  = sess.run(Probs , feed_dict= Feed)\n",
    "    te_real_target = np.squeeze(Test_y.values)\n",
    "    AUC = roc_auc_score(te_real_target , te_probs[:,1])\n",
    "    global best_auc\n",
    "    if best_auc < AUC :\n",
    "        log.info(\"Try : {}, {} < {}\".format(trial.number , best_auc ,AUC))\n",
    "        best_auc= AUC\n",
    "    clear_output()\n",
    "    return AUC\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category : workclass\n",
      "Category : workclass\n",
      "Category : workclass\n",
      "Category : workclass\n",
      "Category : workclassCategory : workclass\n",
      "Category : workclass\n",
      "Category : workclass\n",
      "Category : workclass\n",
      "[workclass] Onehot Shape : [10] --> Embedding Shape : [5] \n",
      "Category : workclass\n",
      "\n",
      "[workclass] Onehot Shape : [10] --> Embedding Shape : [5] Category : education\n",
      "\n",
      "[workclass] Onehot Shape : [10] --> Embedding Shape : [5] \n",
      "Category : educationCategory : education\n",
      "\n",
      "[workclass] Onehot Shape : [10] --> Embedding Shape : [5] \n",
      "[workclass] Onehot Shape : [10] --> Embedding Shape : [5] [workclass] Onehot Shape : [10] --> Embedding Shape : [5] \n",
      "\n",
      "[workclass] Onehot Shape : [10] --> Embedding Shape : [5] Category : education\n",
      "[workclass] Onehot Shape : [10] --> Embedding Shape : [5] \n",
      "Category : education[workclass] Onehot Shape : [10] --> Embedding Shape : [5] [workclass] Onehot Shape : [10] --> Embedding Shape : [5] \n",
      "[education] Onehot Shape : [17] --> Embedding Shape : [8] \n",
      "\n",
      "\n",
      "\n",
      "Category : educationCategory : education\n",
      "[education] Onehot Shape : [17] --> Embedding Shape : [8] \n",
      "Category : education\n",
      "Category : educationCategory : education\n",
      "[education] Onehot Shape : [17] --> Embedding Shape : [8] Category : marital-status\n",
      "\n",
      "\n",
      "\n",
      "Category : marital-status\n",
      "Category : marital-status\n",
      "[education] Onehot Shape : [17] --> Embedding Shape : [8] \n",
      "[education] Onehot Shape : [17] --> Embedding Shape : [8] \n",
      "Category : marital-statusCategory : marital-status\n",
      "\n",
      "[education] Onehot Shape : [17] --> Embedding Shape : [8] [education] Onehot Shape : [17] --> Embedding Shape : [8] \n",
      "[marital-status] Onehot Shape : [8] --> Embedding Shape : [4] \n",
      "[education] Onehot Shape : [17] --> Embedding Shape : [8] \n",
      "[marital-status] Onehot Shape : [8] --> Embedding Shape : [4] \n",
      "[education] Onehot Shape : [17] --> Embedding Shape : [8] Category : marital-status\n",
      "\n",
      "Category : marital-status\n",
      "Category : occupation[education] Onehot Shape : [17] --> Embedding Shape : [8] Category : occupation[marital-status] Onehot Shape : [8] --> Embedding Shape : [4] Category : marital-status\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Category : marital-status\n",
      "Category : occupationCategory : marital-status\n",
      "\n",
      "[marital-status] Onehot Shape : [8] --> Embedding Shape : [4] \n",
      "[marital-status] Onehot Shape : [8] --> Embedding Shape : [4] \n",
      "Category : occupation\n",
      "Category : occupation\n",
      "[marital-status] Onehot Shape : [8] --> Embedding Shape : [4] \n",
      "[occupation] Onehot Shape : [16] --> Embedding Shape : [8] Category : occupation\n",
      "[marital-status] Onehot Shape : [8] --> Embedding Shape : [4] \n",
      "[marital-status] Onehot Shape : [8] --> Embedding Shape : [4] [occupation] Onehot Shape : [16] --> Embedding Shape : [8] \n",
      "\n",
      "Category : relationship[marital-status] Onehot Shape : [8] --> Embedding Shape : [4] \n",
      "\n",
      "\n",
      "Category : occupationCategory : occupation[occupation] Onehot Shape : [16] --> Embedding Shape : [8] Category : relationship\n",
      "\n",
      "\n",
      "\n",
      "[marital-status] Onehot Shape : [8] --> Embedding Shape : [4] Category : occupation\n",
      "\n",
      "Category : relationship\n",
      "Category : occupation\n",
      "[occupation] Onehot Shape : [16] --> Embedding Shape : [8] \n",
      "Category : relationship[occupation] Onehot Shape : [16] --> Embedding Shape : [8] \n",
      "\n",
      "Category : relationship\n",
      "[relationship] Onehot Shape : [7] --> Embedding Shape : [3] [occupation] Onehot Shape : [16] --> Embedding Shape : [8] \n",
      "\n",
      "[occupation] Onehot Shape : [16] --> Embedding Shape : [8] [occupation] Onehot Shape : [16] --> Embedding Shape : [8] \n",
      "\n",
      "Category : relationshipCategory : race[relationship] Onehot Shape : [7] --> Embedding Shape : [3] \n",
      "\n",
      "\n",
      "Category : relationshipCategory : relationship\n",
      "\n",
      "[occupation] Onehot Shape : [16] --> Embedding Shape : [8] [relationship] Onehot Shape : [7] --> Embedding Shape : [3] Category : race\n",
      "\n",
      "\n",
      "[occupation] Onehot Shape : [16] --> Embedding Shape : [8] [relationship] Onehot Shape : [7] --> Embedding Shape : [3] \n",
      "\n",
      "Category : relationshipCategory : race\n",
      "\n",
      "Category : relationshipCategory : race\n",
      "\n",
      "[relationship] Onehot Shape : [7] --> Embedding Shape : [3] \n",
      "Category : race\n",
      "[relationship] Onehot Shape : [7] --> Embedding Shape : [3] \n",
      "[race] Onehot Shape : [6] --> Embedding Shape : [3] Category : race\n",
      "\n",
      "[relationship] Onehot Shape : [7] --> Embedding Shape : [3] [relationship] Onehot Shape : [7] --> Embedding Shape : [3] Category : sex\n",
      "\n",
      "\n",
      "Category : race\n",
      "[race] Onehot Shape : [6] --> Embedding Shape : [3] \n",
      "Category : race\n",
      "Category : sex[sex] Onehot Shape : [3]\n",
      "\n",
      "[race] Onehot Shape : [6] --> Embedding Shape : [3] [relationship] Onehot Shape : [7] --> Embedding Shape : [3] \n",
      "\n",
      "[relationship] Onehot Shape : [7] --> Embedding Shape : [3] \n",
      "[race] Onehot Shape : [6] --> Embedding Shape : [3] [race] Onehot Shape : [6] --> Embedding Shape : [3] \n",
      "\n",
      "Category : sexCategory : race[sex] Onehot Shape : [3]Category : race\n",
      "\n",
      "\n",
      "\n",
      "Category : sexCategory : sexCategory : native-country\n",
      "\n",
      "\n",
      "[race] Onehot Shape : [6] --> Embedding Shape : [3] [sex] Onehot Shape : [3]\n",
      "\n",
      "Category : sexCategory : native-country[sex] Onehot Shape : [3][sex] Onehot Shape : [3]\n",
      "\n",
      "\n",
      "[race] Onehot Shape : [6] --> Embedding Shape : [3] \n",
      "[race] Onehot Shape : [6] --> Embedding Shape : [3] \n",
      "\n",
      "Category : native-country[sex] Onehot Shape : [3]Category : sex\n",
      "Category : sex\n",
      "\n",
      "\n",
      "Category : native-country\n",
      "Category : native-country\n",
      "[sex] Onehot Shape : [3][race] Onehot Shape : [6] --> Embedding Shape : [3] \n",
      "[sex] Onehot Shape : [3][race] Onehot Shape : [6] --> Embedding Shape : [3] \n",
      "\n",
      "\n",
      "Category : sexCategory : native-country\n",
      "\n",
      "[native-country] Onehot Shape : [43] --> Embedding Shape : [21] Category : sex\n",
      "\n",
      "Category : native-country\n",
      "[sex] Onehot Shape : [3]Category : native-country[64, 60, 28, 2]\n",
      "\n",
      "\n",
      "[sex] Onehot Shape : [3][native-country] Onehot Shape : [43] --> Embedding Shape : [21] \n",
      "\n",
      "[native-country] Onehot Shape : [43] --> Embedding Shape : [21] \n",
      "Category : native-country\n",
      "[44, 63, 27, 2]\n",
      "[55, 54, 13, 2]\n",
      "Category : native-country\n",
      "[native-country] Onehot Shape : [43] --> Embedding Shape : [21] \n",
      "[native-country] Onehot Shape : [43] --> Embedding Shape : [21] \n",
      "[65, 41, 23, 2]\n",
      "[53, 59, 19, 2]\n",
      "[native-country] Onehot Shape : [43] --> Embedding Shape : [21] \n",
      "[native-country] Onehot Shape : [43] --> Embedding Shape : [21] \n",
      "[43, 47, 15, 2]\n",
      "[native-country] Onehot Shape : [43] --> Embedding Shape : [21] \n",
      "[45, 35, 18, 2][38, 56, 22, 2]\n",
      "\n",
      "[native-country] Onehot Shape : [43] --> Embedding Shape : [21] \n",
      "[53, 54, 11, 2]\n",
      "[native-country] Onehot Shape : [43] --> Embedding Shape : [21] \n",
      "[56, 38, 20, 2]\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/pandas/core/series.py:1143: FutureWarning:\n",
      "\n",
      "\n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.enable_propagation()\n",
    "optuna.logging.disable_default_handler()\n",
    "sampler = SkoptSampler(\n",
    "    skopt_kwargs={'n_random_starts':0,\n",
    "                  'acq_func':'EI',\n",
    "                  'acq_func_kwargs': {'xi':0.02, \n",
    "                                      \"x0\" : None, \n",
    "                                      \"y0\" : None}})\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().info(\"Start optimization.\")\n",
    "    study = optuna.create_study(direction='maximize',\n",
    "                               sampler = sampler)\n",
    "    study.optimize(objective, \n",
    "                   n_trials=100,\n",
    "                   n_jobs = 10\n",
    "                  )\n",
    "    print('Number of finished trials: ', len(study.trials))\n",
    "\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('  Value: ', trial.value)\n",
    "\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print('    {}: {}'.format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
