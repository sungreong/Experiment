{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd , sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import auc , roc_auc_score\n",
    "sys.path.append(\"/home/advice/Python/SR/Custom/\")\n",
    "from RAdam import RAdamOptimizer\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.2\n"
     ]
    }
   ],
   "source": [
    "import skopt\n",
    "print(skopt.__version__)\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import *\n",
    "def get_weight_variable(shape, name=None,\n",
    "                        type='xavier_uniform', regularize=True, **kwargs):\n",
    "    initialise_from_constant = False\n",
    "    if type == 'xavier_uniform':\n",
    "        initial = xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "    elif type == 'xavier_normal':\n",
    "        initial = xavier_initializer(uniform=False, dtype=tf.float32)\n",
    "    elif type == 'he_normal':\n",
    "        initial = variance_scaling_initializer(uniform=False, factor=2.0, mode='FAN_IN', dtype=tf.float32)\n",
    "    elif type == 'he_uniform':\n",
    "        initial = variance_scaling_initializer(uniform=True, factor=2.0, mode='FAN_IN', dtype=tf.float32)\n",
    "    elif type == 'caffe_uniform':\n",
    "        initial = variance_scaling_initializer(uniform=True, factor=1.0, mode='FAN_IN', dtype=tf.float32)\n",
    "    elif type == 'simple':\n",
    "        stddev = kwargs.get('stddev', 0.02)\n",
    "        initial = tf.truncated_normal(shape, stddev=stddev, dtype=tf.float32)\n",
    "        initialise_from_constant = True\n",
    "    elif type == 'bilinear':\n",
    "        weights = _bilinear_upsample_weights(shape)\n",
    "        initial = tf.constant(weights, shape=shape, dtype=tf.float32)\n",
    "        initialise_from_constant = True\n",
    "    else:\n",
    "        raise ValueError('Unknown initialisation requested: %s' % type)\n",
    "\n",
    "    if name is None:  # This keeps to option open to use unnamed Variables\n",
    "        weight = tf.Variable(initial)\n",
    "    else:\n",
    "        if initialise_from_constant:\n",
    "            weight = tf.get_variable(name, initializer=initial)\n",
    "        else:\n",
    "            weight = tf.get_variable(name, shape=shape, initializer=initial)\n",
    "    if regularize:\n",
    "        tf.add_to_collection('weight_variables', weight)\n",
    "    return weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./../Data/income_evaluation.csv\")\n",
    "objcol = data.select_dtypes(\"object\").columns.tolist()\n",
    "data[objcol] = data[objcol].astype(\"category\")\n",
    "data.columns = [i.strip() for i in data.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"income\"] = data[\"income\"].cat.codes\n",
    "target = data.pop(\"income\")\n",
    "num_col = data.select_dtypes(\"int\").columns.tolist()\n",
    "cat_col = data.select_dtypes(\"category\").columns.tolist()\n",
    "onehot_data = pd.get_dummies(data , columns= cat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Train_X , Test_X , Train_y , Test_y = train_test_split(onehot_data , \n",
    "                                                         target , test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_var = Train_X.columns.tolist()\n",
    "in_var = data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotIndex(in_var , num_var , one_hot_var) :\n",
    "    start_idx = 0\n",
    "    key_store = {}\n",
    "    store = []\n",
    "    for idx , col in enumerate(in_var) :\n",
    "        if col in num_var :\n",
    "            aa = [start_idx , start_idx +1]\n",
    "            store.append(aa)\n",
    "            start_idx += 1\n",
    "        else :\n",
    "            find = [idx for idx , ck in enumerate(one_hot_var) if re.search(\"^{}_\".format(col) , ck)]\n",
    "            nn = len(find)\n",
    "            aa = [start_idx , start_idx + nn]\n",
    "            start_idx += nn\n",
    "            store.append(aa)\n",
    "        key_store[col] = aa\n",
    "    return key_store , store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "key_onehot_store , onehot_store = OneHotIndex(in_var , num_col , one_hot_var)\n",
    "Train_y.reset_index(drop=True ,inplace=True)\n",
    "target_1_list = Train_y[(Train_y == 1) == True].index.tolist()\n",
    "target_0_list = Train_y[(Train_y == 0) == True].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck = list(set(target_1_list) & set(target_0_list))\n",
    "assert ck == [], \"중복 발생 : {}\".format(ck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/root/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/root/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "numeric_features = num_col \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)])\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),])\n",
    "clf.fit(Train_X)\n",
    "Train_X[num_col] = clf.transform(Train_X)\n",
    "#Valid_X[num_col] = clf.transform(Valid_X)\n",
    "Test_X[num_col] = clf.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']\n"
     ]
    }
   ],
   "source": [
    "Train_X_np = Train_X.values\n",
    "#Valid_X_np = Valid_X.values\n",
    "Test_X_np = Test_X.values\n",
    "print(in_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['education', 'race', 'capital-loss', 'hours-per-week', 'capital-gain', 'age', 'education-num', 'native-country', 'occupation', 'sex'], ['capital-gain', 'native-country', 'relationship', 'workclass', 'marital-status', 'hours-per-week', 'sex', 'capital-loss', 'occupation', 'education'], ['occupation', 'workclass', 'education', 'native-country', 'marital-status', 'fnlwgt', 'relationship', 'hours-per-week', 'capital-gain', 'education-num'], ['sex', 'capital-loss', 'workclass', 'fnlwgt', 'occupation', 'race', 'native-country', 'relationship', 'capital-gain', 'education'], ['education', 'capital-gain', 'workclass', 'capital-loss', 'relationship', 'fnlwgt', 'age', 'hours-per-week', 'education-num', 'occupation']]\n"
     ]
    }
   ],
   "source": [
    "def variable_select(in_var = None , method = None , select_n = None , NTree = None) :\n",
    "    var_n = len(in_var)\n",
    "    if method== \"sqrt\" :\n",
    "        value =np.sqrt(var_n)\n",
    "    elif method == \"log2\" :\n",
    "        value =np.log2(var_n)\n",
    "    elif method == \"select\" :\n",
    "        value = select_n\n",
    "    else :\n",
    "        value = var_n\n",
    "    return [list(np.random.choice(in_var ,\n",
    "                                  replace = False , \n",
    "                                  size = value)) for _ in range(NTree)]\n",
    "select_var = variable_select(in_var=in_var , method=\"select\" , \n",
    "                               select_n= 10 , NTree= 5)\n",
    "print(select_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_mish(x) :\n",
    "    act = x * tf.nn.tanh(tf.nn.softplus(x))\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activate_candidate = \\\n",
    "[tf.nn.selu, tf_mish , tf.nn.leaky_relu , tf.nn.elu ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKOPT 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_activation = Categorical(categories=activate_candidate,\n",
    "                             name='activation')\n",
    "\n",
    "dense_layers_1 = Integer(low=2, high=5, name='dense_layers_1')\n",
    "dense_nodes_1 = Integer(low=10, high=28, name='dense_nodes_1')\n",
    "dense_layers_2 = Integer(low=2, high=5, name='dense_layers_2')\n",
    "dense_nodes_2 = Integer(low=10, high=28, name='dense_nodes_2')\n",
    "dense_layers_3 = Integer(low=2, high=5, name='dense_layers_3')\n",
    "dense_nodes_3 = Integer(low=10, high=28, name='dense_nodes_3')\n",
    "dense_layers_4 = Integer(low=2, high=5, name='dense_layers_4')\n",
    "dense_nodes_4 = Integer(low=10, high=28, name='dense_nodes_4')\n",
    "dense_layers_5 = Integer(low=2, high=5, name='dense_layers_5')\n",
    "dense_nodes_5 = Integer(low=10, high=28, name='dense_nodes_5')\n",
    "default_parameters = [1e-5, tf.nn.selu , \n",
    "                      3,15, \n",
    "                      3,15, \n",
    "                      4,10,\n",
    "                      3,25,\n",
    "                      3,20,\n",
    "                     ]\n",
    "dimensions = [\n",
    "    dim_learning_rate ,\n",
    "    dim_activation,\n",
    "    dense_layers_1,\n",
    "    dense_nodes_1,\n",
    "    dense_layers_2,\n",
    "    dense_nodes_2,\n",
    "    dense_layers_3,\n",
    "    dense_nodes_3,\n",
    "    dense_layers_4,\n",
    "    dense_nodes_4,\n",
    "    dense_layers_5,\n",
    "    dense_nodes_5,\n",
    "             ]\n",
    "\n",
    "def log_dir_name(learning_rate, num_dense_layers,\n",
    "                 num_dense_nodes, activation):\n",
    "\n",
    "    # The dir-name for the TensorBoard log-dir.\n",
    "    s = \"./19_logs/lr_{0:.0e}_layers_{1}_nodes_{2}_{3}/\"\n",
    "\n",
    "    # Insert all the hyper-parameters in the dir-name.\n",
    "    log_dir = s.format(learning_rate,\n",
    "                       num_dense_layers,\n",
    "                       num_dense_nodes,\n",
    "                       activation)\n",
    "\n",
    "    return log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22792, 108)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_X_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classifier(Nensemble = select_var ,\n",
    "               learning_rate=None , activation=None,\n",
    "               dense_layers_1 = None , dense_nodes_1 =None , \n",
    "               dense_layers_2 = None , dense_nodes_2 =None , \n",
    "               dense_layers_3 = None , dense_nodes_3 =None , \n",
    "               dense_layers_4 = None , dense_nodes_4 =None , \n",
    "               dense_layers_5 = None , dense_nodes_5 =None , ) :\n",
    "\n",
    "    Ensembles = []\n",
    "    target_n = 2 \n",
    "    hidden = [\n",
    "        [dense_nodes_1] * dense_layers_1 + [target_n] , \n",
    "        [dense_nodes_2] * dense_layers_2 + [target_n] ,\n",
    "        [dense_nodes_3] * dense_layers_3 + [target_n] ,\n",
    "        [dense_nodes_4] * dense_layers_4 + [target_n] ,\n",
    "        [dense_nodes_5] * dense_layers_5 + [target_n]\n",
    "    ]\n",
    "    init_candidate =\\\n",
    "    [\"xavier_uniform\",\"xavier_normal\", \n",
    "     \"he_normal\", \"he_uniform\",\"caffe_uniform\"]\n",
    "    row , dim = Train_X_np.shape\n",
    "    target_n = 2 \n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = [ None , dim],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape = [ None , 1], name=\"y\")\n",
    "    DropoutRate = tf.placeholder(tf.float32, name =\"dropoutRate\")\n",
    "    training = tf.placeholder_with_default(False, shape=[], name=\"training\")\n",
    "    with tf.variable_scope(\"Ensembles\"):\n",
    "        for idx , __vars__ in enumerate(Nensemble) :\n",
    "            x_input = []\n",
    "            for key in __vars__ :\n",
    "                start_node , terminal_node = key_onehot_store[key]\n",
    "                diff = terminal_node - start_node\n",
    "                X_Split = tf.slice(X , [0, start_node] , [-1 , diff])\n",
    "                x_input.append(X_Split)\n",
    "            x_input = tf.concat(x_input , axis = 1)\n",
    "            TOTAL_DIM = x_input.get_shape().as_list()[1]\n",
    "#             SELECT = np.random.randint(0 , len(activate_candidate) , 1)[0]\n",
    "            SELECT2 = np.random.randint(0 , len(init_candidate) , 1)[0]\n",
    "#             activation = activate_candidate[SELECT]\n",
    "            init = init_candidate[SELECT2]\n",
    "            dims = hidden[idx]\n",
    "            dims = [TOTAL_DIM] + dims\n",
    "            print(dims)\n",
    "            for idx2 , h_dim in enumerate(dims) :\n",
    "                if idx2 == 0 :\n",
    "                    Weight =get_weight_variable(shape = [TOTAL_DIM , h_dim], \n",
    "                                                name=\"W_{}{}\".format(idx , idx2),\n",
    "                                                type=init, regularize=True)\n",
    "                    Bias = tf.get_variable(\"Bias_{}{}\".format(idx , idx2),\n",
    "                                           shape = [h_dim] , dtype = tf.float32 , \n",
    "                                           initializer = tf.constant_initializer(0.0))\n",
    "                    Layer = tf.matmul( x_input , Weight) + Bias\n",
    "                    Layer = activation(Layer)\n",
    "                    Weight =get_weight_variable(shape = [TOTAL_DIM , 10], \n",
    "                                                name=\"Info_W_{}{}\".format(idx , idx2),\n",
    "                                                type=init, regularize=True)\n",
    "                    InfoLayer = tf.matmul( x_input , Weight)\n",
    "                    Layer = tf.contrib.nn.alpha_dropout(Layer , DropoutRate ) \n",
    "                else :\n",
    "                    if idx2 == 1 : h_n = dims[idx2-1] + 10\n",
    "                    else : h_n = dims[idx2-1]\n",
    "                    Weight =get_weight_variable(shape = [h_n ,h_dim ], \n",
    "                                                name=\"W_{}{}\".format(idx , idx2),\n",
    "                                                type=init, regularize=True)\n",
    "                    Bias = tf.get_variable(\"Bias_{}{}\".format(idx , idx2),\n",
    "                                           shape = [h_dim] , dtype = tf.float32 , \n",
    "                                           initializer = tf.constant_initializer(0.0))\n",
    "                    if idx2 == 1 : Layer = tf.concat([Layer, InfoLayer], axis = 1)\n",
    "                    else : pass\n",
    "                    Layer = tf.matmul( Layer , Weight) + Bias\n",
    "                    if len(dims) == idx2+1 : \n",
    "                        pass\n",
    "                    else : \n",
    "                        Layer = activation(Layer)\n",
    "                        Layer = tf.contrib.nn.alpha_dropout(Layer , DropoutRate ) \n",
    "            Ensembles.append(Layer)\n",
    "            \n",
    "        NModels = Ensembles\n",
    "        y_one_hot =\\\n",
    "        tf.one_hot( \n",
    "            tf.cast(\n",
    "                tf.squeeze(y , axis = 1 ) , tf.int32) , depth= target_n)\n",
    "        \n",
    "        y_weight_info = compute_class_weight(class_weight= \"balanced\" , \n",
    "                             classes = np.unique(target),\n",
    "                             y= np.squeeze(target))\n",
    "        weight = tf.constant([ y_weight_info[1] ] ) # \n",
    "        mod = sys.modules[__name__]\n",
    "        for idx , Model in enumerate(NModels) :\n",
    "            setattr(mod, 'model_{}_softmax'.format(idx), \n",
    "                    tf.argmax( tf.nn.softmax(Model) , axis = 1 ))\n",
    "        Loss = []\n",
    "        Probs = 0\n",
    "        for idx , Model in enumerate(NModels) :\n",
    "            loss = tf.nn.weighted_cross_entropy_with_logits(targets = y_one_hot ,\n",
    "                                                             logits = Model , \n",
    "                                                             pos_weight = weight)\n",
    "            Probs +=tf.nn.softmax(Model)\n",
    "            Loss.append(loss)\n",
    "            #Loss += loss\n",
    "        Loss = tf.reduce_mean(Loss)\n",
    "        #Loss /= len(NModels)\n",
    "        Probs = tf.nn.softmax(Probs)\n",
    "        vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"Ensembles\")\n",
    "        L2 = []\n",
    "        WEIGHTS = []\n",
    "        import re \n",
    "        for v in vars :\n",
    "            if re.search('W_' , v.name) :\n",
    "                WEIGHTS.append(v)\n",
    "                L2.append(tf.nn.l2_loss(v))\n",
    "        Loss += tf.add_n(L2)  * 0.1\n",
    "        l1_regularizer = tf.contrib.layers.l1_regularizer(scale=0.005, \n",
    "                                                          scope=None)\n",
    "        regularization_penalty = tf.contrib.layers.apply_regularization(l1_regularizer, \n",
    "                                                                        WEIGHTS )\n",
    "        Loss += regularization_penalty\n",
    "        solver = RAdamOptimizer(learning_rate= learning_rate).minimize(Loss ,var_list = vars )\n",
    "        return [Ensembles , Probs , Loss , solver , \n",
    "                X , y , DropoutRate , training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate=None , activation=None,\n",
    "                  dense_layers_1 = None , dense_nodes_1 =None , \n",
    "                  dense_layers_2 = None , dense_nodes_2 =None , \n",
    "                  dense_layers_3 = None , dense_nodes_3 =None , \n",
    "                  dense_layers_4 = None , dense_nodes_4 =None , \n",
    "                  dense_layers_5 = None , dense_nodes_5 =None):\n",
    "    \n",
    "    Lists = Classifier(Nensemble= select_var ,\n",
    "                         learning_rate=learning_rate , \n",
    "                         activation=activation,\n",
    "                         dense_layers_1 = dense_layers_1 ,\n",
    "                         dense_nodes_1 = dense_nodes_1 , \n",
    "                         dense_layers_2 = dense_layers_2 ,\n",
    "                         dense_nodes_2 =dense_nodes_2 , \n",
    "                         dense_layers_3 = dense_layers_3 ,\n",
    "                         dense_nodes_3 =dense_nodes_3 , \n",
    "                         dense_layers_4 = dense_layers_4 ,\n",
    "                         dense_nodes_4 =dense_nodes_4 , \n",
    "                         dense_layers_5 = dense_layers_5 ,\n",
    "                         dense_nodes_5 =dense_nodes_5)\n",
    "    (NModels , Probs ,\n",
    "     Loss , solver,\n",
    "     X , y ,\n",
    "     DropoutRate , training) = Lists\n",
    "    Epoch = 200\n",
    "    mb_size = 1000\n",
    "    config=tf.ConfigProto(log_device_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config = config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    _Loss_ = []\n",
    "    _Epoch_ = []\n",
    "    _Epoch2_ = [0]\n",
    "    _trAUC_ , _teAUC_ = [0] , [0]\n",
    "    for epoch in range(Epoch) :\n",
    "        print(epoch , end = \"\\r\")\n",
    "        target1 = np.random.choice(target_1_list ,\n",
    "                                   int(len(target_1_list)/2) ,\n",
    "                                   replace = False )\n",
    "        target0 = np.random.choice(target_0_list , \n",
    "                                   len(target_0_list) ,\n",
    "                                   replace = True )\n",
    "        target1 = list(target1)\n",
    "        target0 = list(target0)\n",
    "        if epoch > 10 :\n",
    "            target1 = target1 + 2 * checkpoint \n",
    "        XX = Train_X_np[target1 + target0  , : ]\n",
    "        YY = Train_y.values[target1 + target0]\n",
    "        idx = np.random.permutation(len(XX))\n",
    "        XX = XX[idx , : ]\n",
    "        YY = YY[idx]\n",
    "        batch_iter = int(len(XX) / mb_size)\n",
    "        batchLoss = 0\n",
    "        for idx in range(batch_iter) :\n",
    "            X_mb = XX[idx*mb_size:(idx+1)*mb_size]\n",
    "            Y_mb = YY[idx*mb_size:(idx+1)*mb_size]\n",
    "            Feed = {X : X_mb ,\n",
    "                    y : Y_mb.reshape(-1,1) , \n",
    "                    DropoutRate : 0.5 ,\n",
    "                    training : True }\n",
    "            _ , LOSS  = sess.run([solver , Loss] , feed_dict= Feed)\n",
    "            batchLoss += LOSS\n",
    "        batchLoss /= batch_iter\n",
    "        _Loss_.append(batchLoss)\n",
    "        _Epoch_.append(epoch)\n",
    "\n",
    "        if epoch % 10 == 0 :\n",
    "            Feed = {X : Train_X_np  ,\n",
    "                    DropoutRate : 1.0 ,\n",
    "                    training : True }\n",
    "            probs  = sess.run(Probs , feed_dict= Feed)\n",
    "            real_target = np.squeeze(Train_y.values)\n",
    "            pred_target = np.argmax(probs,axis =1)\n",
    "            AUC = roc_auc_score(real_target , probs[:,1])\n",
    "            DD = pd.DataFrame([real_target ,pred_target], index = [\"t\",\"p\"]).T\n",
    "            DD2 = DD[(DD.t == 1) & (DD.p==0)]\n",
    "            checkpoint = DD2.index.tolist()\n",
    "    \n",
    "    \n",
    "    Feed = {X : Train_X_np  ,\n",
    "            DropoutRate : 1.0 ,\n",
    "            training : True }\n",
    "    \n",
    "    probs  = sess.run(Probs , feed_dict= Feed)\n",
    "    real_target = np.squeeze(Train_y.values)\n",
    "    pred_target = np.argmax(probs,axis =1)\n",
    "    AUC = roc_auc_score(real_target , probs[:,1])\n",
    "    \n",
    "    global best_auc\n",
    "    if best_auc < AUC :\n",
    "        print(\"=\"*10)\n",
    "        print(\"{} < {}\".format(best_auc ,AUC))\n",
    "        print(\"=\"*10)\n",
    "        best_auc= AUC\n",
    "    tf.reset_default_graph()\n",
    "    clear_output()\n",
    "    return -AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_result = gp_minimize(func=fitness,\n",
    "                        dimensions=dimensions,\n",
    "                        n_calls= 10,\n",
    "                        acq_func='EI',\n",
    "                        n_random_starts  = 5 , \n",
    "                        noise= 0.01,\n",
    "                        n_jobs= -1,\n",
    "                        kappa = 1,\n",
    "                        x0=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skopt 저장해두고 쓰는 법](https://scikit-optimize.github.io/notebooks/interruptible-optimization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 16, 16, 16, 2]\n",
      "[100, 21, 21, 21, 21, 2]\n",
      "[99, 13, 13, 13, 13, 13, 2]\n",
      "[98, 12, 12, 2]\n",
      "[52, 10, 10, 10, 2]\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:1 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0, compute capability: 6.1\n",
      "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n",
      "133\r"
     ]
    }
   ],
   "source": [
    "x0 = gp_result.x_iters\n",
    "y0 = gp_result.func_vals\n",
    "gp_result = gp_minimize(func=fitness,\n",
    "                        dimensions=dimensions,\n",
    "                        x0 = x0,\n",
    "                        y0 = y0 ,\n",
    "                        n_calls= 100,\n",
    "                        acq_func='EI',\n",
    "                        noise= 0.01,\n",
    "                        n_jobs=-1,\n",
    "                        kappa = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gp_result.func_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gp_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(gp_result.x_iters)\n",
    "list_of_lists = [[f\"nodes{i}\" , f\"layer{i}\"] for i in range(1,6)]\n",
    "layer_node = [y for x in list_of_lists for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = pd.DataFrame(gp_result.x_iters, \n",
    "                      columns=[\"lr\", \"activation\"] + layer_node)\n",
    "Result[\"activation\"] = Result[\"activation\"].apply(lambda x : x.__name__)\n",
    "Result[\"AUC\"] = -gp_result.func_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result.sort_values([\"AUC\"],ascending=False).to_csv(\"skopt_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>activation</th>\n",
       "      <th>nodes1</th>\n",
       "      <th>layer1</th>\n",
       "      <th>nodes2</th>\n",
       "      <th>layer2</th>\n",
       "      <th>nodes3</th>\n",
       "      <th>layer3</th>\n",
       "      <th>nodes4</th>\n",
       "      <th>layer4</th>\n",
       "      <th>nodes5</th>\n",
       "      <th>layer5</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>elu</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.772994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.744567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>selu</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.735782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>tf_mish</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0.684456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>elu</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.679902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>selu</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0.344389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>elu</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>0.332766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>elu</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.312848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>elu</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>0.297578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>elu</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>0.251205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lr  activation  nodes1  layer1  nodes2  layer2  nodes3  layer3  \\\n",
       "35   0.000005         elu       2      10       4      26       5      19   \n",
       "64   0.000014  leaky_relu       2      14       2      27       2      12   \n",
       "34   0.000001        selu       2      11       3      13       2      23   \n",
       "107  0.000001     tf_mish       3      20       4      27       3      17   \n",
       "69   0.000047         elu       5      17       4      17       2      12   \n",
       "..        ...         ...     ...     ...     ...     ...     ...     ...   \n",
       "73   0.000033        selu       2      23       3      11       5      13   \n",
       "93   0.000001         elu       2      24       5      25       2      26   \n",
       "50   0.000063         elu       3      10       4      11       4      28   \n",
       "24   0.000006         elu       5      28       4      13       5      28   \n",
       "33   0.000044         elu       2      25       4      28       2      16   \n",
       "\n",
       "     nodes4  layer4  nodes5  layer5       AUC  \n",
       "35        4      15       4      10  0.772994  \n",
       "64        5      27       4      10  0.744567  \n",
       "34        3      25       5      10  0.735782  \n",
       "107       2      13       3      18  0.684456  \n",
       "69        5      10       2      10  0.679902  \n",
       "..      ...     ...     ...     ...       ...  \n",
       "73        3      27       2      28  0.344389  \n",
       "93        3      27       5      28  0.332766  \n",
       "50        5      11       3      10  0.312848  \n",
       "24        5      28       5      25  0.297578  \n",
       "33        4      11       3      28  0.251205  \n",
       "\n",
       "[110 rows x 13 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result.sort_values([\"AUC\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(gp_result);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
