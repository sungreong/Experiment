{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd , sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import auc , roc_auc_score\n",
    "sys.path.append(\"/home/advice/Python/SR/Custom/\")\n",
    "from RAdam import RAdamOptimizer\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re , os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning:\n",
      "\n",
      "sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.integration import SkoptSampler\n",
    "import logging\n",
    "import plotly\n",
    "from optuna.logging import get_logger\n",
    "from optuna.structs import TrialState\n",
    "from optuna import type_checking\n",
    "from optuna.visualization.utils import _check_plotly_availability\n",
    "from optuna.visualization.utils import is_available\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import *\n",
    "def get_weight_variable(shape, name=None,\n",
    "                        type='xavier_uniform', regularize=True, **kwargs):\n",
    "    initialise_from_constant = False\n",
    "    if type == 'xavier_uniform':\n",
    "        initial = xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "    elif type == 'xavier_normal':\n",
    "        initial = xavier_initializer(uniform=False, dtype=tf.float32)\n",
    "    elif type == 'he_normal':\n",
    "        initial = variance_scaling_initializer(uniform=False, factor=2.0, mode='FAN_IN', dtype=tf.float32)\n",
    "    elif type == 'he_uniform':\n",
    "        initial = variance_scaling_initializer(uniform=True, factor=2.0, mode='FAN_IN', dtype=tf.float32)\n",
    "    elif type == 'caffe_uniform':\n",
    "        initial = variance_scaling_initializer(uniform=True, factor=1.0, mode='FAN_IN', dtype=tf.float32)\n",
    "    elif type == 'simple':\n",
    "        stddev = kwargs.get('stddev', 0.02)\n",
    "        initial = tf.truncated_normal(shape, stddev=stddev, dtype=tf.float32)\n",
    "        initialise_from_constant = True\n",
    "    elif type == 'bilinear':\n",
    "        weights = _bilinear_upsample_weights(shape)\n",
    "        initial = tf.constant(weights, shape=shape, dtype=tf.float32)\n",
    "        initialise_from_constant = True\n",
    "    else:\n",
    "        raise ValueError('Unknown initialisation requested: %s' % type)\n",
    "\n",
    "    if name is None:  # This keeps to option open to use unnamed Variables\n",
    "        weight = tf.Variable(initial)\n",
    "    else:\n",
    "        if initialise_from_constant:\n",
    "            weight = tf.get_variable(name, initializer=initial)\n",
    "        else:\n",
    "            weight = tf.get_variable(name, shape=shape, initializer=initial)\n",
    "    if regularize:\n",
    "        tf.add_to_collection('weight_variables', weight)\n",
    "    return weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./../Data/income_evaluation.csv\")\n",
    "objcol = data.select_dtypes(\"object\").columns.tolist()\n",
    "data[objcol] = data[objcol].astype(\"category\")\n",
    "data.columns = [i.strip() for i in data.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"income\"] = data[\"income\"].cat.codes\n",
    "target = data.pop(\"income\")\n",
    "num_col = data.select_dtypes(\"int\").columns.tolist()\n",
    "cat_col = data.select_dtypes(\"category\").columns.tolist()\n",
    "onehot_data = pd.get_dummies(data , columns= cat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Train_X , Test_X , Train_y , Test_y = train_test_split(onehot_data , \n",
    "                                                         target , test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_var = Train_X.columns.tolist()\n",
    "in_var = data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_norm(w, iteration=1 , name = None):\n",
    "    w_shape = w.shape.as_list()\n",
    "    w = tf.reshape(w, [-1, w_shape[-1]])\n",
    "    u = tf.get_variable(name , [1, w_shape[-1]], \n",
    "                        initializer=tf.random_normal_initializer(), trainable=False)\n",
    "    u_hat = u\n",
    "    v_hat = None\n",
    "    for i in range(iteration):\n",
    "        v_ = tf.matmul(u_hat, tf.transpose(w))\n",
    "        v_hat = tf.nn.l2_normalize(v_)\n",
    "\n",
    "        u_ = tf.matmul(v_hat, w)\n",
    "        u_hat = tf.nn.l2_normalize(u_)\n",
    "\n",
    "    u_hat = tf.stop_gradient(u_hat)\n",
    "    v_hat = tf.stop_gradient(v_hat)\n",
    "\n",
    "    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))\n",
    "\n",
    "    with tf.control_dependencies([u.assign(u_hat)]):\n",
    "        w_norm = w / sigma\n",
    "        w_norm = tf.reshape(w_norm, w_shape)\n",
    "    return w_norm \n",
    "\n",
    "def OneHotIndex(in_var , num_var , one_hot_var) :\n",
    "    start_idx = 0\n",
    "    key_store = {}\n",
    "    store = []\n",
    "    for idx , col in enumerate(in_var) :\n",
    "        if col in num_var :\n",
    "            aa = [start_idx , start_idx +1]\n",
    "            store.append(aa)\n",
    "            start_idx += 1\n",
    "        else :\n",
    "            find = [idx for idx , ck in enumerate(one_hot_var) if re.search(\"^{}_\".format(col) , ck)]\n",
    "            nn = len(find)\n",
    "            aa = [start_idx , start_idx + nn]\n",
    "            start_idx += nn\n",
    "            store.append(aa)\n",
    "        key_store[col] = aa\n",
    "    return key_store , store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "key_onehot_store , onehot_store = OneHotIndex(in_var , num_col , one_hot_var)\n",
    "Train_y.reset_index(drop=True ,inplace=True)\n",
    "target_1_list = Train_y[(Train_y == 1) == True].index.tolist()\n",
    "target_0_list = Train_y[(Train_y == 0) == True].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck = list(set(target_1_list) & set(target_0_list))\n",
    "assert ck == [], \"중복 발생 : {}\".format(ck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/root/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/root/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "numeric_features = num_col \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)])\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),])\n",
    "clf.fit(Train_X)\n",
    "Train_X[num_col] = clf.transform(Train_X)\n",
    "#Valid_X[num_col] = clf.transform(Valid_X)\n",
    "Test_X[num_col] = clf.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']\n"
     ]
    }
   ],
   "source": [
    "Train_X_np = Train_X.values\n",
    "#Valid_X_np = Valid_X.values\n",
    "Test_X_np = Test_X.values\n",
    "print(in_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(in_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hours-per-week', 'age', 'education-num', 'marital-status', 'occupation', 'workclass', 'sex', 'education', 'relationship', 'fnlwgt'], ['education', 'capital-gain', 'relationship', 'marital-status', 'age', 'fnlwgt', 'native-country', 'hours-per-week', 'race', 'education-num'], ['marital-status', 'capital-gain', 'relationship', 'capital-loss', 'sex', 'occupation', 'native-country', 'education-num', 'race', 'age'], ['race', 'native-country', 'sex', 'capital-loss', 'marital-status', 'fnlwgt', 'education-num', 'age', 'hours-per-week', 'occupation'], ['occupation', 'native-country', 'workclass', 'fnlwgt', 'education-num', 'relationship', 'capital-loss', 'marital-status', 'race', 'sex']]\n"
     ]
    }
   ],
   "source": [
    "def variable_select(in_var = None , method = None , select_n = None , NTree = None) :\n",
    "    var_n = len(in_var)\n",
    "    if method== \"sqrt\" :\n",
    "        value =np.sqrt(var_n)\n",
    "    elif method == \"log2\" :\n",
    "        value =np.log2(var_n)\n",
    "    elif method == \"select\" :\n",
    "        value = select_n\n",
    "    else :\n",
    "        value = var_n\n",
    "    return [list(np.random.choice(in_var ,\n",
    "                                  replace = False , \n",
    "                                  size = value)) for _ in range(NTree)]\n",
    "select_var = variable_select(in_var=in_var , method=\"select\" , \n",
    "                               select_n= 10 , NTree= 5)\n",
    "print(select_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_mish(x) :\n",
    "    act = x * tf.nn.tanh(tf.nn.softplus(x))\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activate_candidate = \\\n",
    "[tf.nn.selu, tf_mish , tf.nn.leaky_relu , tf.nn.elu ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [optuna](https://github.com/optuna/optuna/blob/master/examples/tensorflow_eager_simple.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22792, 108)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_X_np.shape\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9, 10, 11, 12, 13]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "list(np.arange(8,14))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(Nensemble = select_var , trial = None) :\n",
    "    layers_1 = trial.suggest_int(\"layers_1\",2,4)\n",
    "    layers_2 = trial.suggest_int(\"layers_2\",2,4)\n",
    "    layers_3 = trial.suggest_int(\"layers_3\",2,4)\n",
    "    layers_4 = trial.suggest_int(\"layers_4\",2,4)\n",
    "    layers_5 = trial.suggest_int(\"layers_5\",2,4)\n",
    "    nodes_1 = trial.suggest_int(\"nodes_1\",30,65)\n",
    "    nodes_2 = trial.suggest_int(\"nodes_2\",30,65)\n",
    "    nodes_3 = trial.suggest_int(\"nodes_3\",30,65)\n",
    "    nodes_4 = trial.suggest_int(\"nodes_4\",30,65)\n",
    "    nodes_5 = trial.suggest_int(\"nodes_5\",30,65)\n",
    "    \n",
    "    init_candidate =\\\n",
    "    [\"xavier_uniform\",\"xavier_normal\", \n",
    "     \"he_normal\", \"he_uniform\",\"caffe_uniform\"]\n",
    "    activate_candidate = \\\n",
    "    [tf.nn.selu, tf_mish , tf.nn.leaky_relu , tf.nn.elu , tf.nn.relu ]\n",
    "    activate_int = [0,1,2,3,4]\n",
    "    Ensembles = []\n",
    "    target_n = 2 \n",
    "    hidden = [\n",
    "        [nodes_1] * layers_1 + [target_n] , \n",
    "        [nodes_2] * layers_2 + [target_n] ,\n",
    "        [nodes_3] * layers_3 + [target_n] ,\n",
    "        [nodes_4] * layers_4 + [target_n] ,\n",
    "        [nodes_5] * layers_5 + [target_n]\n",
    "    ]\n",
    "    \n",
    "    init =\\\n",
    "    trial.suggest_categorical('weight_init', init_candidate)\n",
    "    row , dim = Train_X_np.shape\n",
    "    selected =\\\n",
    "    trial.suggest_categorical('activation', activate_int)\n",
    "    activation = activate_candidate[selected]\n",
    "    X = tf.placeholder(tf.float32, shape = [ None , dim],name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape = [ None , 1], name=\"y\")\n",
    "    DropoutRate = tf.placeholder(tf.float32, name =\"dropoutRate\")\n",
    "    training = tf.placeholder_with_default(False, shape=[], name=\"training\")\n",
    "    with tf.variable_scope(f'Ensembles_{trial.number}' , reuse = tf.AUTO_REUSE):\n",
    "        matrix = []\n",
    "        for idx , __vars__ in enumerate(Nensemble) :\n",
    "            with tf.variable_scope(f\"Tree{idx}\" , reuse = tf.AUTO_REUSE) :\n",
    "                x_input = []\n",
    "                for key in __vars__ :\n",
    "                    start_node , terminal_node = key_onehot_store[key]\n",
    "                    diff = terminal_node - start_node\n",
    "                    X_Split = tf.slice(X , [0, start_node] , [-1 , diff],\n",
    "                                       name = f\"Tree{idx}_{key}\")\n",
    "                    x_input.append(X_Split)\n",
    "                x_input = tf.concat(x_input , axis = 1, name = f\"Tree{idx}_Concat\")\n",
    "                TOTAL_DIM = x_input.get_shape().as_list()[1]\n",
    "                dims = [TOTAL_DIM] + hidden[idx]\n",
    "                matrix.append(dims)\n",
    "                for idx2 , h_dim in enumerate(dims) :\n",
    "                    if idx2 == 0 :\n",
    "                        Weight =get_weight_variable(shape = [TOTAL_DIM , h_dim], \n",
    "                                                    name=f\"W_{idx}{idx2}\",\n",
    "                                                    type=init, regularize=True)\n",
    "                        Bias = tf.get_variable(f\"BIAS_{idx}{idx2}\",\n",
    "                                               shape = [h_dim] , dtype = tf.float32 , \n",
    "                                               initializer = tf.constant_initializer(0.0))\n",
    "                        tf.summary.histogram(f\"Weight{idx2}\", Weight)\n",
    "                        tf.summary.histogram(f\"Bias{idx2}\", Bias)\n",
    "                        \n",
    "                        Layer = tf.matmul( x_input , Weight) + Bias\n",
    "                        Layer = activation(Layer)\n",
    "                        Weight =get_weight_variable(shape = [TOTAL_DIM , 10], \n",
    "                                                    name=f\"W_Info_{idx}{idx2}\",\n",
    "                                                    type=init, regularize=True)\n",
    "                        InfoLayer = tf.matmul( x_input , Weight)\n",
    "                        Layer = tf.contrib.nn.alpha_dropout(Layer , DropoutRate ) \n",
    "                    else :\n",
    "                        if idx2 == 1 : \n",
    "                            h_n = dims[idx2-1] + 10\n",
    "                        else : \n",
    "                            h_n = dims[idx2-1]\n",
    "                        Weight =get_weight_variable(shape = [h_n ,h_dim ], \n",
    "                                                    name=f\"W_{idx}{idx2}\",\n",
    "                                                    type=init, regularize=True)\n",
    "                        Bias = tf.get_variable(f\"BIAS_{idx}{idx2}\",\n",
    "                                               shape = [h_dim] , dtype = tf.float32 , \n",
    "                                               initializer = tf.constant_initializer(0.0))\n",
    "                        if idx2 == 1 : \n",
    "                            Layer = tf.concat([Layer, InfoLayer], axis = 1)\n",
    "                        else : pass\n",
    "                        if len(dims) == idx2+1 : \n",
    "                            Layer = tf.matmul( Layer , Weight) + Bias\n",
    "                        else : \n",
    "#                             Weight = spectral_norm(Weight , name = f\"SNW_{idx}{idx2}\")\n",
    "                            Layer = tf.matmul( Layer , Weight) + Bias\n",
    "                            Layer = activation(Layer)\n",
    "                            Layer = tf.contrib.nn.alpha_dropout(Layer , DropoutRate ) \n",
    "                        tf.summary.histogram(f\"Weight{idx2}\", Weight)\n",
    "                        tf.summary.histogram(f\"Bias{idx2}\", Bias)\n",
    "            Ensembles.append(Layer)\n",
    "        \n",
    "        NModels = Ensembles\n",
    "        y_one_hot =\\\n",
    "        tf.one_hot( \n",
    "            tf.cast(\n",
    "                tf.squeeze(y , axis = 1 ) , tf.int32) , depth= target_n)\n",
    "        \n",
    "        y_weight_info = compute_class_weight(class_weight= \"balanced\" , \n",
    "                             classes = np.unique(target),\n",
    "                             y= np.squeeze(target))\n",
    "        weight = tf.constant([ y_weight_info[1] ] ) # \n",
    "        mod = sys.modules[__name__]\n",
    "        for idx , Model in enumerate(NModels) :\n",
    "            setattr(mod, 'model_{}_softmax'.format(idx), \n",
    "                    tf.argmax( tf.nn.softmax(Model) , axis = 1 ))\n",
    "        Loss = []\n",
    "        Probs = 0\n",
    "        for idx , Model in enumerate(NModels) :\n",
    "            loss = tf.nn.weighted_cross_entropy_with_logits(targets = y_one_hot ,\n",
    "                                                             logits = Model , \n",
    "                                                             pos_weight = weight)\n",
    "            Probs +=tf.nn.softmax(Model)\n",
    "            Loss.append(loss)\n",
    "            #Loss += loss\n",
    "        Loss = tf.reduce_mean(Loss)\n",
    "        #Loss /= len(NModels)\n",
    "        Probs = tf.nn.softmax(Probs)\n",
    "        tf.summary.histogram(f\"Probability\", Probs)\n",
    "        vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"Ensembles\")\n",
    "        L2 = []\n",
    "        WEIGHTS = []\n",
    "        \n",
    "        for v in vars :\n",
    "            if re.search('W_' , v.name) :\n",
    "                WEIGHTS.append(v)\n",
    "                L2.append(tf.nn.l2_loss(v))\n",
    "        Loss += tf.add_n(L2)  * 0.01\n",
    "        l1_regularizer = tf.contrib.layers.l1_regularizer(scale=0.005, \n",
    "                                                          scope=None)\n",
    "        regularization_penalty = tf.contrib.layers.apply_regularization(l1_regularizer, \n",
    "                                                                        WEIGHTS )\n",
    "        Loss += regularization_penalty\n",
    "        kwargs = {}\n",
    "#         optimizer_options = ['RMSPropOptimizer', 'AdamOptimizer', 'MomentumOptimizer',\"RADAM\"]\n",
    "#         optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "#         if optimizer_selected == 'RMSPropOptimizer':\n",
    "#             kwargs['learning_rate'] = trial.suggest_loguniform('rmsprop_learning_rate', 1e-5, 1e-1)\n",
    "#             kwargs['decay'] = trial.suggest_uniform('rmsprop_decay', 0.85, 0.99)\n",
    "#             kwargs['momentum'] = trial.suggest_loguniform('rmsprop_momentum', 1e-5, 1e-1)\n",
    "#             optimizer = getattr(tf.train, optimizer_selected)(**kwargs)\n",
    "#         elif optimizer_selected == 'AdamOptimizer':\n",
    "#             kwargs['learning_rate'] = trial.suggest_loguniform('adam_learning_rate', 1e-5, 1e-1)\n",
    "#             optimizer = getattr(tf.train, optimizer_selected)(**kwargs)\n",
    "#         elif optimizer_selected == 'MomentumOptimizer':\n",
    "#             kwargs['learning_rate'] = trial.suggest_loguniform('momentum_opt_learning_rate',\n",
    "#                                                                1e-5,\n",
    "#                                                                1e-1)\n",
    "#             kwargs['momentum'] = trial.suggest_loguniform('momentum_opt_momentum', 1e-5, 1e-1)\n",
    "#             optimizer = getattr(tf.train, optimizer_selected)(**kwargs)\n",
    "#         else :\n",
    "        kwargs['learning_rate'] = trial.suggest_loguniform('Radam_learning_rate', 1e-5, 1e-1)\n",
    "        optimizer = RAdamOptimizer(**kwargs)\n",
    "        solver = optimizer.minimize(Loss ,var_list = vars )\n",
    "        \n",
    "        prediction = tf.argmax(Probs, 1)\n",
    "        correct = tf.argmax(y_one_hot, 1)\n",
    "        equality = tf.equal(prediction, correct)\n",
    "        accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "        tf.summary.scalar(f\"Accuracy_{trial.number}\",accuracy)\n",
    "        tf.summary.scalar(f\"Loss_{trial.number}\",Loss)\n",
    "        merged = tf.summary.merge_all()\n",
    "        logging.getLogger().info(\"Try : {} \\n {}\".format(trial.number,\n",
    "                                                     str(matrix)))\n",
    "        return [Ensembles , Probs , Loss , solver, \n",
    "                X , y , DropoutRate , training,merged]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = 0.0\n",
    "def objective(trial):   \n",
    "#     selected =\\\n",
    "#     trial.suggest_categorical('select_n', np.arange(8,14).astype(int).tolist())\n",
    "#     select_var = variable_select(in_var=in_var , \n",
    "#                                  method=\"select\" , \n",
    "#                                  select_n= selected , NTree= 5)\n",
    "    Lists = create_model(Nensemble = select_var,\n",
    "                         trial =trial)\n",
    "    \n",
    "    (NModels , Probs ,\n",
    "     Loss , solver,\n",
    "     X , y ,\n",
    "     DropoutRate , training,\n",
    "     merged\n",
    "    ) = Lists\n",
    "    Epoch = 500\n",
    "    mb_size = 500\n",
    "    \n",
    "    \n",
    "    config=tf.ConfigProto(\n",
    "        log_device_placement=True,\n",
    "#         intra_op_parallelism_threads=8,\n",
    "#         inter_op_parallelism_threads=4\n",
    "    )\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config = config)\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter(f'./OptunaResult/train_{trial.number}',\n",
    "                                         sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    _Loss_ = []\n",
    "    _Epoch_ = []\n",
    "    _Epoch2_ = [0]\n",
    "    _trAUC_ , _teAUC_ = [0] , [0]\n",
    "    \n",
    "    \n",
    "    for epoch in range(Epoch) :\n",
    "        target1 = np.random.choice(target_1_list ,\n",
    "                                   int(len(target_1_list)/2) ,\n",
    "                                   replace = False )\n",
    "        target0 = np.random.choice(target_0_list , \n",
    "                                   len(target_0_list) ,\n",
    "                                   replace = True )\n",
    "        target1 = list(target1)\n",
    "        target0 = list(target0)\n",
    "#         if epoch > 10 :\n",
    "#             target1 = target1 + checkpoint \n",
    "        XX = Train_X_np[target1 + target0  , : ]\n",
    "        YY = Train_y.values[target1 + target0]\n",
    "        idx = np.random.permutation(len(XX))\n",
    "        XX = XX[idx , : ]\n",
    "        YY = YY[idx]\n",
    "        batch_iter = int(len(XX) / mb_size)\n",
    "        batchLoss = 0\n",
    "        for idx in range(batch_iter) :\n",
    "            X_mb = XX[idx*mb_size:(idx+1)*mb_size]\n",
    "            Y_mb = YY[idx*mb_size:(idx+1)*mb_size]\n",
    "            Feed = {X : X_mb ,\n",
    "                    y : Y_mb.reshape(-1,1) , \n",
    "                    DropoutRate : 0.5 ,\n",
    "                    training : True }\n",
    "            _ , LOSS =\\\n",
    "            sess.run([solver , Loss] ,\n",
    "                     feed_dict= Feed\n",
    "                    )\n",
    "            \n",
    "            batchLoss += LOSS\n",
    "        batchLoss /= batch_iter\n",
    "        _Loss_.append(batchLoss)\n",
    "        _Epoch_.append(epoch)\n",
    "        \n",
    "        \n",
    "        if epoch % 10 == 9 :\n",
    "#             run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "#             run_metadata = tf.RunMetadata()\n",
    "            Feed = {X : X_mb ,\n",
    "                    y : Y_mb.reshape(-1,1) , \n",
    "                    DropoutRate : 0.5 ,\n",
    "                    training : True }\n",
    "            _ , summary, LOSS =\n",
    "            sess.run([solver , merged, Loss] ,\n",
    "                     feed_dict= Feed,)\n",
    "#                      options=run_options,\n",
    "#                      run_metadata=run_metadata\n",
    "#             train_writer.add_run_metadata(run_metadata, \n",
    "#                                           'epoch%03d' % epoch)\n",
    "            train_writer.add_summary(summary, epoch)\n",
    "            \n",
    "        if epoch % 100 == 0 :\n",
    "            Feed = {X : Train_X_np  ,\n",
    "                    DropoutRate : 0.5 ,\n",
    "                    training : True }\n",
    "            probs  = sess.run(Probs , \n",
    "                              feed_dict= Feed,)\n",
    "            real_target = np.squeeze(Train_y.values)\n",
    "            pred_target = np.argmax(probs,axis =1)\n",
    "            AUC = roc_auc_score(real_target , probs[:,1])\n",
    "#             DD = pd.DataFrame([real_target,\n",
    "#                                pred_target], \n",
    "#                               index = [\"t\",\"p\"]).T\n",
    "#             DD2 = DD[(DD.t == 1) & (DD.p==0)]\n",
    "#             checkpoint = DD2.index.tolist()\n",
    "            if epoch % 100 == 0 :\n",
    "                msg = \"Try : {:3}, Epoch : {:3}, AUC : {:.2f}%\".\\\n",
    "            format(trial.number, epoch , 100*AUC)\n",
    "                logging.getLogger().info(msg)\n",
    "    train_writer.close()\n",
    "    Feed = {X : Train_X_np  ,\n",
    "            DropoutRate : 0.5 ,\n",
    "            training : True }\n",
    "    probs  = sess.run(Probs , feed_dict= Feed)\n",
    "    real_target = np.squeeze(Train_y.values)\n",
    "    pred_target = np.argmax(probs,axis =1)\n",
    "    AUC = roc_auc_score(real_target , probs[:,1])\n",
    "    \n",
    "    global best_auc\n",
    "    if best_auc < AUC :\n",
    "        logging.getLogger().info(\"Try : {}, {} < {}\".format(trial.number , best_auc ,AUC))\n",
    "        best_auc= AUC\n",
    "    tf.reset_default_graph()\n",
    "    clear_output()\n",
    "    return AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(logging.FileHandler('optuna.txt', mode=\"w\"))\n",
    "optuna.logging.enable_propagation()\n",
    "optuna.logging.disable_default_handler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x0 = [[3, 2, 4, 3, 4, 36, 32, 49, 40, 35, 'he_uniform', 3, 0.00015671897437797408], \n",
    "      [3, 4, 2, 2, 3, 34, 64, 45, 54, 40, 'he_normal', 3, 1.4147691642608397e-05], \n",
    "      [4, 3, 3, 4, 2, 46, 65, 62, 42, 57, 'xavier_uniform', 0, 0.0001151939657356718], [3, 4, 2, 4, 4, 60, 34, 54, 63, 40, 'he_uniform', 3, 0.00023751936361984937], [11, 2, 4, 3, 4, 3, 38, 58, 39, 31, 63, 'caffe_uniform', 4, 1.185261441167407e-05], [12, 2, 2, 2, 2, 3, 43, 54, 42, 56, 30, 'caffe_uniform', 3, 2.8053303983617267e-05], [13, 3, 4, 3, 4, 2, 53, 31, 57, 31, 40, 'he_uniform', 1, 0.004342367249195597], [10, 3, 2, 2, 2, 2, 52, 56, 50, 40, 45, 'caffe_uniform', 2, 0.0002353657442834687], [12, 2, 3, 2, 2, 4, 37, 43, 38, 48, 56, 'xavier_normal', 2, 1.1695403014796976e-05], [10, 4, 3, 2, 4, 4, 62, 55, 56, 30, 44, 'xavier_uniform', 2, 0.00132122127636011], [12, 2, 4, 4, 3, 3, 64, 47, 60, 39, 64, 'xavier_normal', 3, 0.002818043271032938], [11, 2, 4, 3, 3, 4, 54, 34, 35, 49, 63, 'xavier_uniform', 1, 0.00011653454036311457], [12, 4, 2, 3, 4, 3, 51, 45, 36, 41, 40, 'caffe_uniform', 1, 0.0013094508034332806], [9, 3, 2, 4, 4, 3, 62, 51, 64, 36, 42, 'xavier_normal', 3, 0.006172462319995353], [8, 2, 3, 4, 2, 2, 65, 38, 38, 53, 65, 'xavier_uniform', 1, 0.004848434779819864], [8, 3, 2, 3, 2, 4, 65, 65, 60, 30, 30, 'caffe_uniform', 2, 0.09999999999999999], [10, 4, 4, 4, 3, 3, 65, 34, 65, 30, 46, 'xavier_normal', 3, 0.09999999999999999], [8, 2, 4, 3, 3, 4, 61, 53, 43, 52, 46, 'caffe_uniform', 3, 0.09999999999999999], [8, 4, 3, 4, 3, 2, 63, 57, 44, 55, 48, 'he_uniform', 1, 0.09999999999999999], [8, 2, 3, 4, 2, 3, 36, 30, 30, 63, 32, 'he_normal', 2, 0.09999999999999999], [8, 2, 3, 4, 2, 2, 60, 39, 41, 65, 33, 'xavier_uniform', 4, 0.0002012076028834959], [11, 3, 3, 4, 3, 4, 35, 63, 63, 62, 64, 'xavier_uniform', 1, 0.00866714545089891], [8, 2, 3, 3, 4, 4, 55, 56, 63, 60, 34, 'he_normal', 2, 0.00010933878687689759], [8, 2, 4, 3, 4, 4, 48, 40, 52, 60, 32, 'he_normal', 2, 0.09999999999999999], [8, 4, 4, 3, 3, 4, 60, 54, 65, 41, 32, 'he_normal', 3, 1.0054555508216113e-05], [8, 3, 3, 3, 3, 2, 31, 57, 60, 51, 30, 'he_normal', 1, 0.0026168135968490723], [8, 4, 4, 4, 3, 3, 37, 64, 62, 33, 52, 'he_normal', 0, 0.01685645874697779], [11, 4, 3, 3, 3, 4, 61, 47, 54, 55, 47, 'he_normal', 1, 0.0003765011074475807], [10, 2, 3, 3, 3, 3, 63, 52, 56, 58, 34, 'caffe_uniform', 3, 0.09999999999999999], [8, 2, 2, 4, 2, 2, 30, 30, 30, 65, 30, 'he_normal', 2, 0.09999999999999999], [8, 2, 2, 3, 2, 3, 30, 30, 30, 53, 32, 'he_normal', 0, 0.09999999999999999], [8, 4, 3, 3, 4, 3, 48, 63, 47, 34, 61, 'caffe_uniform', 0, 0.0006988342189973038], [8, 2, 2, 4, 2, 3, 48, 50, 38, 60, 44, 'he_normal', 3, 0.0010630558921353032], [8, 4, 3, 3, 3, 3, 55, 63, 37, 31, 51, 'xavier_uniform', 0, 0.040182145025863716], [8, 4, 3, 2, 4, 3, 52, 36, 65, 36, 54, 'caffe_uniform', 2, 1.799227078888316e-05], [8, 2, 2, 3, 3, 3, 30, 42, 46, 53, 53, 'he_normal', 3, 0.0017274481636200155], [8, 4, 2, 2, 4, 3, 39, 42, 58, 58, 34, 'caffe_uniform', 0, 0.02538727255814498], [8, 3, 2, 3, 2, 3, 34, 64, 54, 32, 44, 'caffe_uniform', 0, 0.05841467788043324], [8, 3, 2, 2, 4, 3, 30, 40, 36, 40, 35, 'caffe_uniform', 0, 2.6425141972924894e-05], [8, 4, 2, 3, 4, 3, 53, 50, 65, 54, 65, 'xavier_uniform', 2, 0.00035018555480494817], [8, 4, 2, 2, 4, 3, 60, 61, 64, 38, 56, 'xavier_uniform', 4, 0.00030709124496748716], [8, 2, 2, 3, 2, 3, 30, 53, 53, 56, 64, 'he_normal', 3, 0.056749864882203925], [8, 2, 2, 4, 2, 3, 30, 65, 59, 30, 65, 'xavier_uniform', 0, 0.09999999999999999], [9, 2, 2, 4, 3, 3, 57, 60, 56, 50, 63, 'xavier_uniform', 1, 2.1659583232534686e-05], [13, 2, 2, 4, 2, 3, 30, 65, 65, 30, 62, 'he_normal', 0, 0.007582478981301236], [8, 2, 2, 4, 2, 3, 30, 65, 65, 30, 65, 'he_normal', 0, 0.09999999999999999], [9, 3, 2, 4, 2, 3, 45, 65, 65, 30, 30, 'he_normal', 2, 0.09999999999999999], [8, 2, 2, 4, 2, 3, 41, 65, 30, 30, 65, 'he_normal', 2, 0.011195733193718348], [8, 2, 2, 4, 2, 3, 65, 65, 43, 30, 37, 'he_normal', 2, 0.015092488426198855], [8, 4, 2, 3, 2, 3, 63, 65, 47, 34, 43, 'he_normal', 2, 1e-05], [8, 4, 2, 2, 3, 3, 57, 55, 65, 42, 65, 'he_normal', 2, 1e-05], [8, 2, 3, 3, 4, 3, 34, 62, 47, 38, 64, 'he_normal', 0, 1.0017455593748346e-05], [10, 3, 2, 4, 2, 3, 54, 65, 39, 46, 50, 'he_normal', 0, 0.0010848883882730042], [13, 2, 3, 2, 2, 3, 57, 65, 36, 56, 55, 'he_normal', 0, 1e-05], [12, 3, 2, 2, 2, 3, 65, 53, 65, 35, 30, 'he_normal', 0, 1e-05], [13, 4, 3, 2, 4, 3, 47, 43, 53, 37, 61, 'he_normal', 4, 2.3448490928467227e-05], [8, 4, 3, 2, 2, 3, 43, 42, 31, 51, 48, 'caffe_uniform', 0, 8.923072180908639e-05], [13, 2, 2, 2, 3, 3, 36, 65, 30, 33, 64, 'he_normal', 0, 2.5008788649091418e-05], [13, 2, 3, 4, 2, 3, 55, 34, 41, 41, 33, 'he_normal', 0, 6.300417507628337e-05], [13, 3, 3, 4, 2, 3, 46, 49, 36, 36, 59, 'he_normal', 0, 1.4168106849205638e-05], [13, 3, 3, 4, 2, 3, 44, 57, 51, 57, 44, 'he_normal', 0, 0.047648161089890124], [10, 3, 3, 4, 2, 3, 53, 58, 48, 63, 52, 'he_normal', 0, 0.0006616156670531221], [11, 3, 3, 2, 2, 3, 49, 56, 61, 50, 35, 'he_normal', 0, 0.003874206631357768], [11, 4, 3, 2, 4, 3, 30, 35, 49, 30, 48, 'he_normal', 0, 0.09999999999999999], [13, 2, 3, 2, 4, 3, 45, 65, 30, 40, 39, 'caffe_uniform', 0, 1.3124990909659663e-05], [11, 4, 3, 2, 2, 3, 59, 60, 36, 51, 65, 'xavier_uniform', 0, 0.016798014117541147], [13, 4, 3, 2, 2, 3, 59, 51, 37, 48, 65, 'xavier_uniform', 0, 0.00010245200147978441], [13, 4, 4, 2, 2, 3, 56, 65, 30, 30, 65, 'he_normal', 0, 1e-05], [8, 4, 4, 2, 2, 3, 32, 58, 60, 39, 51, 'he_normal', 0, 1.138181205622081e-05], [8, 4, 2, 2, 2, 3, 41, 54, 30, 30, 65, 'he_normal', 0, 1e-05], [8, 4, 2, 2, 2, 3, 32, 55, 30, 30, 65, 'he_normal', 0, 1e-05], [13, 3, 3, 2, 2, 3, 38, 48, 56, 36, 65, 'caffe_uniform', 2, 7.267730586124988e-05], [13, 3, 3, 2, 2, 3, 55, 58, 42, 34, 65, 'he_normal', 2, 1.960429089732139e-05], [8, 4, 4, 3, 2, 3, 46, 61, 48, 36, 38, 'he_normal', 2, 0.03533183513155234], [8, 4, 4, 3, 2, 3, 45, 40, 64, 60, 41, 'he_normal', 1, 0.0017356418135645042], [8, 3, 4, 3, 2, 3, 58, 43, 47, 38, 40, 'he_normal', 0, 0.0009185683862714557], [8, 3, 4, 3, 2, 3, 62, 51, 57, 40, 42, 'he_normal', 0, 0.0003828493855941364]]\n",
    "y0 = [0.5, 0.37312627079078414, 0.5, 0.5, 0.40127449514370905, 0.5, 0.5, 0.5, 0.3953523854056861, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.33482347489277114, 0.5194659099702321, 0.1966741884180666, 0.42179660969198135, 0.5488758740195663, 0.5, 0.5, 0.5, 0.4357786121775885, 0.32259930095007106, 0.5, 0.5, 0.5, 0.31408737435719747, 0.3165046787537358, 0.5782271547251162, 0.5, 0.5, 0.5000287786347415, 0.5, 0.5, 0.5, 0.5033644497408181, 0.5, 0.5, 0.5, 0.4474929984736806, 0.45493217463262253, 0.5, 0.5, 0.5785984352326466, 0.31545481099947675, 0.5, 0.5, 0.5561970460903911, 0.3749490185795035, 0.439759109548035, 0.5, 0.6167678019492814, 0.28535717674560535, 0.5, 0.5, 0.5, 0.5, 0.5611044938310887, 0.5258427040774896, 0.5, 0.5, 0.24205781919861497, 0.5, 0.5000913182518414, 0.5, 0.38260363612366477, 0.700793203550933, 0.507285148316521, 0.4964040570034959, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "\n",
    "sampler = SkoptSampler(\n",
    "    skopt_kwargs={'n_random_starts':0,\n",
    "                  'acq_func':'EI',\n",
    "                  'acq_func_kwargs': {'xi':0.02, \n",
    "                                      \"x0\" : None, \n",
    "                                      \"y0\" : None}})\n",
    "#optuna.logging.enable_default_handler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 13:51:07.922246 140701369960192 study.py:628] Finished trial#2 resulted in value: 0.5011268274490706. Current best value is 0.5011268274490706 with parameters: {'layers_1': 2, 'layers_2': 2, 'layers_3': 3, 'layers_4': 4, 'layers_5': 3, 'nodes_1': 58, 'nodes_2': 61, 'nodes_3': 36, 'nodes_4': 41, 'nodes_5': 42, 'weight_init': 'he_normal', 'activation': 4, 'Radam_learning_rate': 0.0027976533870262537}.\n",
      "I0126 13:51:13.657022 140701369960192 <ipython-input-20-261486b73ed5>:169] Try : 5 \n",
      " [[59, 63, 63, 63, 63, 2], [81, 30, 30, 30, 30, 2], [81, 61, 61, 61, 61, 2], [76, 37, 37, 2], [89, 36, 36, 2]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0126 13:51:24.960213 140701369960192 <ipython-input-21-1700dc0c0155>:107] Try :   5, Epoch :   0, AUC : 48.83%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    logging.getLogger().info(\"Start optimization.\")\n",
    "    study = optuna.create_study(direction='maximize',\n",
    "                               sampler = sampler)\n",
    "    study.optimize(objective, \n",
    "                   n_trials=100,\n",
    "                   n_jobs = 5\n",
    "                  )\n",
    "    print('Number of finished trials: ', len(study.trials))\n",
    "\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('  Value: ', trial.value)\n",
    "\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print('    {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.trials_dataframe().sort_values([\"value\"],ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = []\n",
    "y0 = []\n",
    "for i in range(len(study.get_trials())) :\n",
    "    check = study.get_trials()[i].value\n",
    "    if check is None :\n",
    "        continue\n",
    "    x0.append(list(study.get_trials()[i].params.values()))\n",
    "    y0.append(study.get_trials()[i].value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x0)\n",
    "print(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_available():\n",
    "    from optuna.visualization.plotly_imports import go\n",
    "\n",
    "#optuna.visualization.plot_intermediate_values(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna.visualization.plot_contour(study, params=['layers_1', 'layers_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna.visualization.plot_slice(study, params['layers_1', 'layers_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna.visualization.plot_parallel_coordinate(study, params=['layers_1', 'layers_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
