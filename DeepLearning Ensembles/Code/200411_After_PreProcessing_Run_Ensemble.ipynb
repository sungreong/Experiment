{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd , sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import auc , roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append(\"/home/advice/Python/SR/Custom/PreProcessingUtils/\")\n",
    "import seaborn as sns\n",
    "import re , os\n",
    "from ColMatch import MatchVariable\n",
    "from TabularDataPipeline import *\n",
    "from pre_utils import make_dirs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn(\"this will not show\")\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultPath = \"./Result/Preprocessing_test\"\n",
    "folder_names = make_dirs(ResultPath , local_path=[\"weight_importance\", \"nnTree\"])\n",
    "ResultPath , WeightPath , nnTreePath = folder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./../../../Data/kdd/uci/uci_creditcard-train-0.2-0.0.csv\")\n",
    "in_var = [\"LIMIT_BAL\", \"SEX\", \"EDUCATION\", \"MARRIAGE\", \"AGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\",\n",
    "          \"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\",\n",
    "          \"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\"]\n",
    "target_var = ['default payment next month']\n",
    "fac_var = [ 'SEX','EDUCATION','MARRIAGE',]\n",
    "num_var = [i for i in in_var if not i in fac_var]\n",
    "#in_var = num_var + fac_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.sep_idx ==1 ]\n",
    "valid = data[data.sep_idx ==0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./../../../Data/kdd/uci/uci_creditcard-test-0.2-0.0.csv\")\n",
    "# test = test.drop([\"ID\",\"sep_idx\"],axis=1)\n",
    "# test.reset_index(drop=True ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21018, 26), (8982, 26), (6004, 26))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please insert config type in ['Outlier', 'NumericImpute', 'FactorImpute', 'Numeric', 'Category', 'NewVar']\n"
     ]
    }
   ],
   "source": [
    "pipe = TabularPipeline(in_var, num_var, target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {\n",
    "#     \"Outlier\" : {\"method\" : \"IQR\", \"multiple_outlier_n\" :4 , \"var\": num_var} , \n",
    "    \"NumericImpute\" : {\"method\" : \"mean\", \"var\" : num_var},\n",
    "    \"FactorImpute\" : {\"method\" : \"most_frequent\", \"var\" : fac_var},\n",
    "    \"Numeric\" : {\"method\" : \"sklearn:minmax\", \"var\" : num_var , \n",
    "                 \"method_option\" : {\"feature_range\" : (-1.5,1.5)}},\n",
    "    \"Category\" : {\"method\" : \"labelencoding\", \"dummy_na\" : False} , \n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.insert(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'sklearn:minmax', 'var': ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'], 'method_option': {'feature_range': (-1.5, 1.5)}}\n",
      "Build PipeLine\n"
     ]
    }
   ],
   "source": [
    "pipe.Steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Missing Data / Method : mean\n",
      "Transform Missing Data / Method : most_frequent\n",
      "Transform Numeric Data Method : ('sklearn:minmax', \"{'feature_range': (-1.5, 1.5)}\")\n",
      "Fit Category Handler\n",
      "['ID', 'sep_idx', 'default payment next month']\n",
      "Fitting!\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Pipeline path : ./Result/Preprocessing_test/pipe.pkl\n",
      "Load Pipeline\n"
     ]
    }
   ],
   "source": [
    "save_pipe_path = f\"{ResultPath}/pipe.pkl\"\n",
    "ReusePipe = BADSPipeLine(save_pipe_path)\n",
    "ReusePipe.save(pipe.PIPE)\n",
    "ReusePipe.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Missing Data / Method : mean\n",
      "Transform Missing Data / Method : most_frequent\n",
      "Transform Numeric Data Method : ('sklearn:minmax', \"{'feature_range': (-1.5, 1.5)}\")\n",
      "Transform Categorical Data / method : labelencoding\n",
      "Transform Missing Data / Method : mean\n",
      "Transform Missing Data / Method : most_frequent\n",
      "Transform Numeric Data Method : ('sklearn:minmax', \"{'feature_range': (-1.5, 1.5)}\")\n",
      "Transform Categorical Data / method : labelencoding\n",
      "Transform Missing Data / Method : mean\n",
      "Transform Missing Data / Method : most_frequent\n",
      "Transform Numeric Data Method : ('sklearn:minmax', \"{'feature_range': (-1.5, 1.5)}\")\n",
      "Transform Categorical Data / method : labelencoding\n"
     ]
    }
   ],
   "source": [
    "train = ReusePipe.transform(train)\n",
    "valid = ReusePipe.transform(valid)\n",
    "test = ReusePipe.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x ,train_y , _ = ReusePipe.split(train, numpy= True)\n",
    "valid_x , valid_y , _ = ReusePipe.split(valid, numpy= True)\n",
    "test_x , test_y , _ = ReusePipe.split(test, numpy= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_select(in_var = None , method = None , select_n = None , NTree = None) :\n",
    "    var_n = len(in_var)\n",
    "    if method== \"sqrt\" :\n",
    "        value =np.sqrt(var_n)\n",
    "    elif method == \"log2\" :\n",
    "        value =np.log2(var_n)\n",
    "    elif method == \"select\" :\n",
    "        value = select_n\n",
    "    else :\n",
    "        value = var_n\n",
    "    output = [\n",
    "        list(np.random.choice(np.arange(len(in_var)) ,\n",
    "                              replace = False , size = value)) for _ in range(NTree)]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT_N = 3\n",
    "select_var = variable_select(in_var=in_var , method=\"select\" , \n",
    "                               select_n= 17 ,NTree= SELECT_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_mish(x) :\n",
    "    return x * tf.nn.tanh(tf.nn.softplus(x))\n",
    "activate_candidate = \\\n",
    "[tf.nn.selu, tf_mish , tf.nn.elu , tf.nn.relu , tf.nn.swish]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerForEnsemble(object) :\n",
    "    def __init__(self , fac_var,category_info) : \n",
    "        self.fac_var = fac_var\n",
    "        self.info = category_info\n",
    "    def run(self, X , select_var, name, type=\"embedding\", emb_dim = 4) :\n",
    "        inputs = []\n",
    "        for idx in select_var :\n",
    "            split = tf.slice(X, [0, idx], [-1, 1])\n",
    "            if idx in list(self.info.keys()) :\n",
    "                split =tf.cast(split, dtype=tf.int32)\n",
    "                split = tf.reshape(split,(-1,))\n",
    "                size = max(list(self.info[idx].values())) + 1\n",
    "                if type == \"embedding\" :    \n",
    "                    Cat = tf.keras.layers.Embedding(size,emb_dim)(split)\n",
    "                elif type == \"onehot\" :\n",
    "                    Cat = tf.one_hot(split ,depth=size)\n",
    "                else :\n",
    "                    raise Exception(f\"No Valid Type : {type}, Please Change the type to onehot or embedding\")\n",
    "                inputs.append(Cat)\n",
    "            else :\n",
    "                inputs.append(split)\n",
    "        x_input = tf.concat(inputs , axis = 1, name=name)\n",
    "        return x_input\n",
    "def Layer(X , hdims, name , activation,\n",
    "          bn_istraining,DropoutRate,reuse = False) :\n",
    "    with tf.variable_scope(f\"EnsLayer_{name}\",reuse=reuse):\n",
    "        for idx2 , h_dim in enumerate(hdims) :\n",
    "            if idx2 == 0 :\n",
    "                W = tf.get_variable(f\"w_{idx2}\",shape=[X.get_shape()[1],h_dim])\n",
    "                B = tf.get_variable(f\"b_{idx2}\",shape=[h_dim])\n",
    "                layer = tf.matmul(X, W) + B\n",
    "            else :\n",
    "                W = tf.get_variable(f\"w_{idx2}\",shape=[hdims[idx2-1] ,h_dim ])\n",
    "                B = tf.get_variable(f\"b_{idx2}\",shape=[h_dim])    \n",
    "                layer = tf.matmul(layer, W) + B\n",
    "            if len(hdims) == idx2 + 1 :\n",
    "                logit = layer\n",
    "            else :\n",
    "                layer = tf.layers.\\\n",
    "                batch_normalization(layer, center=True, \n",
    "                                    scale=True, training=bn_istraining)\n",
    "                layer = activation(layer)\n",
    "                layer = tf.nn.dropout(layer, keep_prob=DropoutRate)\n",
    "    return logit\n",
    "    \n",
    "def EnsembleNN(X , hidden = [[],[]], bn_istraining=None,\n",
    "               DropoutRate=None, Combs=None , RepLayer= None) :\n",
    "    Ensembles = []\n",
    "    with tf.variable_scope(\"NNEnsemble\") :\n",
    "        for idx , Comb in enumerate(Combs) :\n",
    "            x_input = RepLayer.run(X, Comb, f\"nnTree{idx}\",\"embedding\",3)\n",
    "            X_DIM = x_input.get_shape().as_list()[1]\n",
    "            dims = hidden[idx]\n",
    "            dims = [X_DIM] + dims\n",
    "            print(f\"No.{idx} nnTree : {str(dims)}\")\n",
    "            SELECT = np.random.randint(0 , len(activate_candidate) , 1)[0]\n",
    "            activation = activate_candidate[SELECT]\n",
    "            LAYER = Layer(x_input , dims, idx , activation,\n",
    "                          bn_istraining,DropoutRate)\n",
    "            Ensembles.append(LAYER)\n",
    "    return Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21018 23\n"
     ]
    }
   ],
   "source": [
    "row , dim = train_x.shape\n",
    "print(row,dim)\n",
    "target_n = len(np.unique(train_y)) \n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape = [ None , dim])\n",
    "if target_n == 2 :\n",
    "    y = tf.placeholder(tf.float32, shape = [ None , 1])\n",
    "elif target_n ==1 :\n",
    "    raise Exception(\"only one value\")\n",
    "else :\n",
    "    y = tf.placeholder(tf.float32, shape = [ None , target_n])\n",
    "DropoutRate = tf.placeholder(tf.float32, name =\"dropoutRate\")\n",
    "training = tf.placeholder_with_default(False, shape=[], name=\"training\")\n",
    "global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n",
    "bn_istraining  = tf.placeholder(tf.bool)\n",
    "batch_size = tf.placeholder(tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0411 18:13:34.195964 140528790968064 deprecation.py:323] From <ipython-input-30-a1d5d68f924b>:5: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "data_tuple = (X,y)\n",
    "class_dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n",
    "class_dataset = class_dataset.shuffle(buffer_size= 30000,)\n",
    "class_dataset = class_dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "iter = class_dataset.make_initializable_iterator()\n",
    "feature_x , label_y = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(select_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = pipe.return_trans_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0411 18:13:36.332198 140528790968064 deprecation.py:506] From /root/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0411 18:13:36.345810 140528790968064 deprecation.py:506] From /root/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "W0411 18:13:36.413512 140528790968064 deprecation.py:323] From <ipython-input-28-1b9d9821b7cb>:41: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "W0411 18:13:36.414879 140528790968064 deprecation.py:323] From /root/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "W0411 18:13:36.492155 140528790968064 deprecation.py:506] From <ipython-input-28-1b9d9821b7cb>:43: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.0 nnTree : [21, 21, 10, 5, 1]\n",
      "No.1 nnTree : [23, 15, 8, 1]\n",
      "No.2 nnTree : [21, 25, 10, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "target_ny = int(y.get_shape()[1]) \n",
    "HIDDEN = [\n",
    "    [ 21 , 10, 5, target_ny] , \n",
    "    [ 15 , 8, target_ny] ,\n",
    "    [ 25 , 10, 5, target_ny] ,\n",
    "    [ 15 , 8, target_ny] ,\n",
    "    [ 25 , 10, 5, target_ny]\n",
    "]\n",
    "RepLayer = LayerForEnsemble(fac_var , information[\"cat_info\"])\n",
    "NModels = EnsembleNN(feature_x , hidden = HIDDEN , \n",
    "                     bn_istraining=bn_istraining, \n",
    "                     DropoutRate=DropoutRate,\n",
    "                     Combs= select_var,\n",
    "                     RepLayer = RepLayer\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_control = tf.get_variable(\"weight_control\",shape = [len(NModels)] , \n",
    "                                  dtype = tf.float32 , \n",
    "                                  initializer = tf.constant_initializer(1/len(NModels)),\n",
    "                                 constraint= lambda x: tf.clip_by_value(x, 0.01, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_control = tf.nn.softmax(weights_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_control_dim = tf.expand_dims(weights_control, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "NModels_ = [tf.expand_dims(model,axis=1)  for model in NModels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tf.concat(NModels_,axis=1) * weights_control_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = tf.reduce_sum(models , axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sum:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = tf.nn.weighted_cross_entropy_with_logits(\n",
    "    labels = label_y , logits=logit,\n",
    "    pos_weight=1.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0411 18:13:40.623680 140528790968064 deprecation.py:323] From <ipython-input-42-d403ffd19225>:11: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "Probs = []\n",
    "Losses = []\n",
    "argMax = [ ]\n",
    "for idx , Model in enumerate(NModels) :\n",
    "#     loss2 = tf.nn.\\\n",
    "#     weighted_cross_entropy_with_logits(\n",
    "#         labels = label_y , logits=tf.slice(models , [0,idx],[-1,1]),\n",
    "#         pos_weight=2.0)\n",
    "    PROB = tf.nn.sigmoid(Model)\n",
    "    argmax = tf.where(PROB > tf.constant(0.5), \n",
    "                      tf.ones_like(PROB), tf.zeros_like(PROB))\n",
    "    argMax.append(argmax)\n",
    "    Probs.append(PROB)\n",
    "    Losses.append(loss2)\n",
    "#     Probs += tf.nn.sigmoid(Model)\n",
    "#prob = tf.reshape(tf.reduce_sum(tf.concat(Probs,axis=1)*weights_control,axis=1),(-1,1))\n",
    "#prob = tf.reshape(tf.reduce_sum(tf.concat(NModels,axis=1)*weights_control,axis=1),(-1,1))\n",
    "prob = tf.nn.sigmoid(logit)\n",
    "total_argmax = tf.where(prob > tf.constant(0.5), \n",
    "                      tf.ones_like(prob), tf.zeros_like(prob))\n",
    "argMax.append(total_argmax)\n",
    "# loss2 = tf.reduce_mean(Losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Select_2:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "argMax.append(label_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_pred = tf.equal( total_argmax, label_y )\n",
    "accuracy = tf.reduce_mean(tf.cast(comp_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0411 18:13:42.533451 140528790968064 deprecation.py:323] From /root/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/metrics_impl.py:808: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "auc_value , update_auc = tf.metrics.auc(label_y , prob , curve=\"ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"NNEnsemble\") \n",
    "totalvars = tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.train.cosine_decay_restarts(1e-5, global_step,\n",
    "                                               first_decay_steps=100, t_mul=1.5,m_mul=0.9, alpha=0.0)\n",
    "L2= []\n",
    "for v in totalvars :\n",
    "    L2.append(tf.nn.l2_loss(v))\n",
    "L2Regularizer= tf.add_n(L2)  * 1e-4\n",
    "loss2 +=L2Regularizer\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    solver2 = tf.train.AdamOptimizer(learning_rate= learning_rate).minimize(loss2 ,var_list = totalvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.axes_grid1.inset_locator import (inset_axes, TransformedBbox,\n",
    "                                                   BboxPatch, BboxConnector)\n",
    "def my_mark_inset(parent_axes, inset_axes, loc1a=1, loc1b=1, loc2a=2, loc2b=2, **kwargs):\n",
    "    rect = TransformedBbox(inset_axes.viewLim, parent_axes.transData)\n",
    "    pp = BboxPatch(rect, fill=False, **kwargs)\n",
    "    parent_axes.add_patch(pp)\n",
    "    p1 = BboxConnector(inset_axes.bbox, rect, loc1=loc1a, loc2=loc1b, **kwargs)\n",
    "    inset_axes.add_patch(p1)\n",
    "    p1.set_clip_on(False)\n",
    "    p2 = BboxConnector(inset_axes.bbox, rect, loc1=loc2a, loc2=loc2b, **kwargs)\n",
    "    inset_axes.add_patch(p2)\n",
    "    p2.set_clip_on(False)\n",
    "    return pp, p1, p2\n",
    "def subplotting(ax , store , x , y ,cond={}, **kwargs) :\n",
    "    ax.plot(store[x],store[y],**kwargs)\n",
    "    if \"ylabel\" in cond :\n",
    "        ax.set_ylabel(cond[\"ylabel\"], fontsize= 10)\n",
    "    if \"xlabel\" in cond :\n",
    "        ax.set_xlabel(cond[\"xlabel\"], fontsize= 10)\n",
    "    if \"xlim\" in cond :\n",
    "        ax.set_xlim(cond[\"xlim\"])\n",
    "    if \"ylim\" in cond :\n",
    "        ax.set_ylim(cond[\"ylim\"])\n",
    "    if \"title\" in cond :\n",
    "        ax.set_title(cond[\"title\"], fontsize= 15)\n",
    "    ax.legend()\n",
    "    return ax\n",
    "## version3\n",
    "def vis_onlysl(store:dict, path:str,title:str) :\n",
    "    clear_output()\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    gs = GridSpec(nrows=2, ncols=2)\n",
    "    ax1 = fig.add_subplot(gs[0:1, 0])\n",
    "    ax2 = fig.add_subplot(gs[0:1, 1])\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.99, \n",
    "            top=0.9, wspace=0.3, hspace=0.2)\n",
    "    ax1.plot(store[\"epoch\"],store[\"slloss\"],label = \"SLloss\", color='c')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    select_sl= np.argmin(store[\"slloss\"])\n",
    "    ax1.vlines(store[\"epoch\"][select_sl],\n",
    "               np.min(store[\"slloss\"]),np.max(store[\"slloss\"]),\n",
    "               label='Best', color='c')\n",
    "    a =store[\"slloss\"][-1]\n",
    "    ax1.set_title(f'Loss : {a:.4f}')\n",
    "    ax1.set_ylabel('Supervised', fontsize= 15)\n",
    "    ax1.legend()\n",
    "    ax2.plot(store[\"epoch\"],store[\"auc\"],label = \"Train auc\")\n",
    "    ax2.plot(store[\"epoch\"],store[\"teauc\"],label = \"Test auc\")\n",
    "    ax2.set_ylabel('AUC', fontsize= 15)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    select= np.argmax(store[\"teauc\"])\n",
    "    msg = f\"test idx : {store['epoch'][select]}, maximum : {store['teauc'][select]*100:.2f}\"\n",
    "    ax2.set_title(msg)             \n",
    "#     ax2.set_ylim(0.7,store['auc'][select]+0.1)\n",
    "    ax2.legend()\n",
    "    check_n = 4\n",
    "    if len(store['epoch']) > check_n :\n",
    "        axins = inset_axes(ax2, \"100%\", \"100%\", \n",
    "                           bbox_to_anchor=[0.36, .3, .5, .4],\n",
    "                       bbox_transform=ax2.transAxes, borderpad=0)\n",
    "        axins.plot(store['epoch'], store['auc'])\n",
    "        axins.plot(store['epoch'], store['teauc'])\n",
    "        maximum = np.max(store['auc'][-check_n:] + store['teauc'][-check_n:])\n",
    "        minumum = np.min(store['auc'][-check_n:] + store['teauc'][-check_n:])\n",
    "        xlims = (store['epoch'][-check_n],store['epoch'][-1])\n",
    "        ylims = (minumum, maximum)\n",
    "        axins.set(xlim=xlims, ylim=ylims)\n",
    "        my_mark_inset(ax2, axins, loc1a=2, loc1b=3, loc2a=4, loc2b=4, fc=\"none\", ec=\"0.5\") # \n",
    "#     msg = f\"Epoch : {epoch[-1]}, Loss : {loss[-1]:.3f}, Auc : {aucs[-1]:.3f}\"\n",
    "#     skplt.metrics.plot_ks_statistic(store[\"train_y\"], store[\"train_prob\"], \n",
    "#                                 ax = ax3 ,\n",
    "#                                 title = \"[Train] KS Static PLOT\")\n",
    "#     skplt.metrics.plot_ks_statistic(store[\"test_y\"], store[\"test_prob\"], \n",
    "#                                 ax = ax4 ,\n",
    "#                                 title = \"[Test] KS Static PLOT\")\n",
    "    sns.boxplot(x=\"t\", y=\"prob\", data=store[\"train_pd\"] , ax = ax3)\n",
    "    ax3.set_title(\"train\" , fontsize= 15)\n",
    "    sns.boxplot(x=\"t\", y=\"prob\", data=store[\"test_pd\"] , ax = ax4)\n",
    "    ax4.set_title(\"test\" , fontsize= 15)\n",
    "    plt.suptitle(title)\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "def pcolorplot(result:pd.DataFrame, save_path, title) :\n",
    "    models = np.arange(result.shape[0]-2).tolist()\n",
    "    models = models + [\"pred\",\"real\"]\n",
    "    indexs = np.arange(result.shape[1])\n",
    "    fig, ax = plt.subplots( figsize = (12,8))\n",
    "    plt.pcolor(s)\n",
    "    plt.yticks(np.arange(0.5, len(models), 1),models)\n",
    "    plt.title(title,fontsize=20)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21018"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "Epoch = 100000\n",
    "oversample = list(np.arange(len(train_y))) + 1 * list(np.where(train_y == 1)[0])\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.metrics import auc , roc_auc_score\n",
    "sess =  tf.Session(config=config)\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init)\n",
    "store = {\"epoch\" : [] ,  \"slloss\" : [],\n",
    "         \"auc\" : [], \"teauc\" : []}\n",
    "store[\"train_pd\"] = []\n",
    "store[\"test_pd\"] = []\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 57, SLLoss : 0.898013\r"
     ]
    }
   ],
   "source": [
    "for i in range(i, Epoch):    \n",
    "    batchSlLoss = []\n",
    "    permutation_idx = np.random.permutation(oversample)\n",
    "    sess.run(iter.initializer , \n",
    "             feed_dict= {X : train_x[permutation_idx],\n",
    "                         y : train_y[[permutation_idx]],\n",
    "                         batch_size : 300,\n",
    "                        }) # switch to train dataset\n",
    "    while True :\n",
    "        try :\n",
    "            ### Supervised Learning\n",
    "            result  = sess.run([solver2 , loss2],\n",
    "                               feed_dict={global_step:i, \n",
    "                                          bn_istraining :True,\n",
    "                                          DropoutRate : 0.8\n",
    "                                         })\n",
    "            batchSlLoss.append(result[1])\n",
    "        except tf.errors.OutOfRangeError :\n",
    "            break\n",
    "    mslloss = np.mean(batchSlLoss)\n",
    "    print(f\"Epoch : {i}, SLLoss : {mslloss:.6f}\" , end = \"\\r\")\n",
    "    if (i % 100 == 0) :\n",
    "        sess.run(iter.initializer , \n",
    "                 feed_dict= {X : train_x,\n",
    "                 y : train_y,\n",
    "                 batch_size : len(train_x),}) \n",
    "        tr_result  = sess.run([prob,label_y],\n",
    "                              feed_dict={bn_istraining :False,\n",
    "                                         DropoutRate : 1.0\n",
    "                                        })\n",
    "        sess.run(iter.initializer , \n",
    "                 feed_dict= {X : test_x,\n",
    "                 y : test_y,\n",
    "                 batch_size : len(test_x),}) \n",
    "        result  = sess.run([prob,label_y,weights_control],\n",
    "                           feed_dict={bn_istraining :False,\n",
    "                                      DropoutRate : 1.0\n",
    "                                     })\n",
    "        plt.scatter(np.arange(0,len(result[2])), result[2])\n",
    "        plt.savefig(f\"{WeightPath}/plot.{i:05d}.png\")\n",
    "        a = \"Weight nnTree\" +\" \\n\"\n",
    "        b = a + str(result[2])\n",
    "        plt.title(b)\n",
    "        plt.close()\n",
    "        sess.run(iter.initializer , \n",
    "                 feed_dict= {X : test_x,\n",
    "                 y : test_y,\n",
    "                 batch_size : len(test_x),}) \n",
    "        dd  = sess.run(argMax+[accuracy],\n",
    "                           feed_dict={bn_istraining :False,\n",
    "                                      DropoutRate : 1.0\n",
    "                                     })\n",
    "        s = pd.DataFrame(np.concatenate(dd[:-1],axis=1)).T                \n",
    "        models = np.arange(s.shape[0]-2).tolist()\n",
    "        models = models + [\"pred\",\"real\"]\n",
    "        indexs = np.arange(s.shape[1])\n",
    "        fig, ax = plt.subplots( figsize = (12,8))\n",
    "        plt.pcolor(s)\n",
    "        plt.yticks(np.arange(0.5, len(models), 1),models)\n",
    "        plt.title(f\"Epoch {i} , Auccray : {dd[-1]*100:.2f}\",fontsize=20)\n",
    "        plt.savefig(f\"{nnTreePath}/plot.{i:05d}.png\")\n",
    "        plt.close()  \n",
    "        store[\"epoch\"].append(i)\n",
    "        store[\"slloss\"].append(mslloss)\n",
    "        store[\"train_pd\"] = pd.DataFrame(np.concatenate(tr_result,axis=1),\n",
    "                                         columns=[\"prob\",\"t\"])\n",
    "        store[\"test_pd\"] = pd.DataFrame(np.concatenate(result[0:2],axis=1),\n",
    "                                         columns=[\"prob\",\"t\"])\n",
    "        store[\"auc\"].append(roc_auc_score(store[\"train_pd\"][\"t\"].values,\n",
    "                                          store[\"train_pd\"][\"prob\"].values))\n",
    "        store[\"teauc\"].append(roc_auc_score(store[\"test_pd\"][\"t\"].values,\n",
    "                                            store[\"test_pd\"][\"prob\"].values))\n",
    "#             store[\"train_prob\"] =  np.concatenate((1-tr_result[2],tr_result[2]),axis=1)\n",
    "#             store[\"test_prob\"] =  np.concatenate((1-result[2],result[2]),axis=1)\n",
    "        vis_onlysl(store, f\"{ResultPath}/0405_SL_EnsembelNN.png\",\n",
    "                   title=\"Train Ensemblem Neural Network\")\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in  category_info.items() :\n",
    "    a[key] = pd.Categorical(a[key],list(value.values()))\n",
    "pd.get_dummies(a,columns=fac_var).columnsumns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
