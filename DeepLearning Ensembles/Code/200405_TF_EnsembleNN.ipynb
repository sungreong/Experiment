{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PreProcessingBASD_v2 import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd , sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import auc , roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append(\"/home/advice/Python/SR/Custom/\")\n",
    "from RAdam import RAdamOptimizer\n",
    "import seaborn as sns\n",
    "import re , os\n",
    "from ColumnMatch import MatchVariable\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultPath = \"./Result/Ensemble\"\n",
    "weightResultPath = os.path.join(ResultPath, \"weight\")\n",
    "testPath = os.path.join(ResultPath, \"test\")\n",
    "if tf.io.gfile.exists(ResultPath) :\n",
    "    tf.io.gfile.rmtree(ResultPath)\n",
    "    tf.io.gfile.makedirs(ResultPath)\n",
    "    tf.io.gfile.makedirs(weightResultPath)\n",
    "    tf.io.gfile.makedirs(testPath)\n",
    "else :\n",
    "    tf.io.gfile.makedirs(ResultPath)\n",
    "    tf.io.gfile.makedirs(weightResultPath)\n",
    "    tf.io.gfile.makedirs(testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./../../../Data/kdd/uci/uci_creditcard-train-0.2-0.0.csv\")\n",
    "in_var = [\"LIMIT_BAL\", \"SEX\", \"EDUCATION\", \"MARRIAGE\", \"AGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\",\n",
    "          \"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\",\n",
    "          \"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\"]\n",
    "target_var = ['default payment next month']\n",
    "fac_var = [ 'SEX','EDUCATION','MARRIAGE',]\n",
    "num_var = [i for i in in_var if not i in fac_var]\n",
    "#in_var = num_var + fac_var\n",
    "data[fac_var] = data[fac_var].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SEX', 'EDUCATION', 'MARRIAGE']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_col = data.select_dtypes(\"object\").columns.tolist()\n",
    "cat_col\n",
    "#onehot_data = pd.get_dummies(data , columns= cat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_var = in_var + target_var + [\"sep_idx\"]\n",
    "data = data[select_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.sep_idx ==1 ]\n",
    "valid = data[data.sep_idx ==0 ]\n",
    "## \n",
    "train = data\n",
    "# _ = train.pop(\"sep_idx\")\n",
    "# _ = valid.pop(\"sep_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./../../../Data/kdd/uci/uci_creditcard-test-0.2-0.0.csv\")\n",
    "test[fac_var] = test[fac_var].astype(\"str\")\n",
    "# test = test.drop([\"ID\",\"sep_idx\"],axis=1)\n",
    "# test.reset_index(drop=True ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000, 25), (8982, 25), (6004, 26))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform MissingData Method : mean\n",
      "Transform MissingData Method : most_frequent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('NumericImpute',\n",
       "                 MissingHandling(method=None,\n",
       "                                 trans_info={'fill_value': array([ 1.67484323e+05,  3.54752228e+01, -1.70900886e-02, -1.25958973e-01,\n",
       "       -1.65824567e-01, -2.16004663e-01, -2.64603095e-01, -2.86467414e-01,\n",
       "        5.10673010e+04,  4.95652237e+04,  4.70681142e+04,  4.32569520e+04,\n",
       "        4.04164970e+04,  3.88574464e+04,  5.69172658e+03,  5.81409563e+03...\n",
       "        5.13448505e+03,  4.83411333e+03,  4.76632123e+03,  5.24146976e+03]),\n",
       "                                            'norm_std': array([1.29747662e+05, 8.24146160e+00, 9.99950166e-01, 1.07116176e+00,\n",
       "       1.07345534e+00, 1.04935645e+00, 1.01015089e+00, 1.03371751e+00,\n",
       "       6.54338544e+04, 6.39443098e+04, 6.25215503e+04, 5.72229826e+04,\n",
       "       5.45746221e+04, 5.33084779e+04, 1.51993804e+04, 1.80845825e+04,\n",
       "       1.44245395e+04, 1.38536166e+04, 1.33906631e+04, 1.61330268e+04])}))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_info = {}\n",
    "Steps = [\n",
    "    (\"NumericImpute\", MissingHandling(method=\"mean\", \n",
    "                               trans_info= trans_info,\n",
    "                               var= num_var)) , \n",
    "    (\"FactorImpute\", MissingHandling(method=\"most_frequent\", \n",
    "                               trans_info= trans_info,\n",
    "                               var= fac_var)) , \n",
    "    (\"numeric\", NumericHandler(method=\"normal\",\n",
    "                               trans_info= trans_info,\n",
    "                               num_var=num_var)),\n",
    "]\n",
    "\n",
    "pipe = Pipeline(Steps)\n",
    "pipe.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform MissingData Method : mean\n",
      "Transform MissingData Method : most_frequent\n",
      "Transform NumericData\n",
      "Transform MissingData Method : mean\n",
      "Transform MissingData Method : most_frequent\n",
      "Transform NumericData\n",
      "Transform MissingData Method : mean\n",
      "Transform MissingData Method : most_frequent\n",
      "Transform NumericData\n",
      "Transform MissingData Method : mean\n",
      "Transform MissingData Method : most_frequent\n",
      "Transform NumericData\n"
     ]
    }
   ],
   "source": [
    "pipe_op = BADSPipeLine(\"./0405_Piepeline.pkl\")\n",
    "pipe_op.save(pipe)\n",
    "pipe_op.load()\n",
    "pipe_op.transform(test)\n",
    "\n",
    "train = pipe_op.transform(train)\n",
    "valid = pipe_op.transform(valid)\n",
    "test = pipe_op.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEX          object\n",
       "EDUCATION    object\n",
       "MARRIAGE     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[fac_var].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SEX': {'1.0': 1, '2.0': 2, 'nan': 3}, 'EDUCATION': {'0.0': 1, '1.0': 2, '2.0': 3, '3.0': 4, '4.0': 5, '5.0': 6, '6.0': 7, 'nan': 8}, 'MARRIAGE': {'0.0': 1, '1.0': 2, '2.0': 3, '3.0': 4, 'nan': 5}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "category_info = {}\n",
    "\n",
    "class CategroyLabelEncoding(object) :\n",
    "    def __init__(self , fac_var , in_var) :\n",
    "        self.fac_var = fac_var\n",
    "        self.in_var = in_var\n",
    "        self.category_info = {}\n",
    "    \n",
    "    def fit(self, data) :\n",
    "        for feat in self.fac_var:\n",
    "            findIDX = [idx for idx, i in enumerate(self.in_var) if i == feat][0]\n",
    "            lbe = LabelEncoder()\n",
    "            lbe.fit(data[feat].values)\n",
    "            diz_map_train = dict(zip(lbe.classes_, lbe.transform(lbe.classes_) + 1))\n",
    "            self.category_info[feat] = diz_map_train\n",
    "        return \"label encoding\"\n",
    "    \n",
    "    def transform(self,data) :\n",
    "        for feat in self.fac_var:\n",
    "            findIDX = [idx for idx, i in enumerate(self.in_var) if i == feat][0]\n",
    "            current_key = list(self.category_info[feat].keys())\n",
    "            remain_set = set(data[feat]).difference(current_key)\n",
    "            if remain_set == set() :\n",
    "                pass\n",
    "            else :\n",
    "                for i in remain_set :  ## if not in train impose 0\n",
    "                    self.category_info[feat].update({i : 0})\n",
    "#         data[feat] = [diz_map_train[i] for i in data[feat].values]\n",
    "        print(self.category_info)\n",
    "        data2 = data.replace(self.category_info)\n",
    "        for key, value in  self.category_info.items() :\n",
    "            data2[key] = pd.Categorical(data2[key],list(value.values()))\n",
    "        return data2\n",
    "\n",
    "    \n",
    "testclass = CategroyLabelEncoding(fac_var , in_var)\n",
    "testclass.fit(train)\n",
    "train = testclass.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SEX': {'1.0': 1, '2.0': 2, 'nan': 3}, 'EDUCATION': {'0.0': 1, '1.0': 2, '2.0': 3, '3.0': 4, '4.0': 5, '5.0': 6, '6.0': 7, 'nan': 8}, 'MARRIAGE': {'0.0': 1, '1.0': 2, '2.0': 3, '3.0': 4, 'nan': 5}}\n",
      "{'SEX': {'1.0': 1, '2.0': 2, 'nan': 3}, 'EDUCATION': {'0.0': 1, '1.0': 2, '2.0': 3, '3.0': 4, '4.0': 5, '5.0': 6, '6.0': 7, 'nan': 8}, 'MARRIAGE': {'0.0': 1, '1.0': 2, '2.0': 3, '3.0': 4, 'nan': 5}}\n"
     ]
    }
   ],
   "source": [
    "valid = testclass.transform(valid)\n",
    "test = testclass.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_info = testclass.category_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SEX': {'1.0': 1, '2.0': 2, 'nan': 3},\n",
       " 'EDUCATION': {'0.0': 1,\n",
       "  '1.0': 2,\n",
       "  '2.0': 3,\n",
       "  '3.0': 4,\n",
       "  '4.0': 5,\n",
       "  '5.0': 6,\n",
       "  '6.0': 7,\n",
       "  'nan': 8},\n",
       " 'MARRIAGE': {'0.0': 1, '1.0': 2, '2.0': 3, '3.0': 4, 'nan': 5}}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X_np = train[in_var].values\n",
    "Train_y = train[target_var[0]]\n",
    "Test_X_np = test[in_var].values\n",
    "Test_y = test[target_var[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n"
     ]
    }
   ],
   "source": [
    "print(in_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[in_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'1.0': 1, '2.0': 2, 'nan': 3},\n",
       " 2: {'0.0': 1,\n",
       "  '1.0': 2,\n",
       "  '2.0': 3,\n",
       "  '3.0': 4,\n",
       "  '4.0': 5,\n",
       "  '5.0': 6,\n",
       "  '6.0': 7,\n",
       "  'nan': 8},\n",
       " 3: {'0.0': 1, '1.0': 2, '2.0': 3, '3.0': 4, 'nan': 5}}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx , name in enumerate(in_var) :\n",
    "    if name in list(cat_info.keys()) :\n",
    "        cat_info[idx] = cat_info.pop(name)\n",
    "cat_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5280152"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(pd.get_dummies(train, columns= fac_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4890840"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_select(in_var = None , method = None , select_n = None , NTree = None) :\n",
    "    var_n = len(in_var)\n",
    "    if method== \"sqrt\" :\n",
    "        value =np.sqrt(var_n)\n",
    "    elif method == \"log2\" :\n",
    "        value =np.log2(var_n)\n",
    "    elif method == \"select\" :\n",
    "        value = select_n\n",
    "    else :\n",
    "        value = var_n\n",
    "    return [list(np.random.choice(np.arange(len(in_var)) ,\n",
    "                                  replace = False , \n",
    "                                  size = value)) for _ in range(NTree)]\n",
    "SELECT_N = 7\n",
    "select_var = variable_select(in_var=in_var , method=\"select\" , \n",
    "                               select_n= 10 ,NTree= SELECT_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 9, 5, 21, 16, 6, 4, 15, 8, 20],\n",
       " [3, 6, 18, 9, 10, 14, 20, 16, 15, 12],\n",
       " [16, 2, 18, 9, 12, 6, 20, 10, 17, 4],\n",
       " [16, 11, 13, 6, 17, 0, 21, 1, 19, 8],\n",
       " [17, 1, 20, 21, 19, 10, 2, 13, 18, 0],\n",
       " [7, 0, 18, 5, 21, 22, 14, 20, 17, 6],\n",
       " [6, 9, 16, 15, 8, 18, 17, 13, 5, 19]]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd , sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'1.0': 1, '2.0': 2, 'nan': 3},\n",
       " 2: {'0.0': 1,\n",
       "  '1.0': 2,\n",
       "  '2.0': 3,\n",
       "  '3.0': 4,\n",
       "  '4.0': 5,\n",
       "  '5.0': 6,\n",
       "  '6.0': 7,\n",
       "  'nan': 8},\n",
       " 3: {'0.0': 1, '1.0': 2, '2.0': 3, '3.0': 4, 'nan': 5}}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([3]),\n",
       " array([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
       "        21, 22])]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = np.arange(len(in_var))\n",
    "fac_idx = list(cat_info.keys())\n",
    "split = list(set(np.array(fac_idx)) | set(np.array(fac_idx ) + 1))\n",
    "lists = np.split(total , split)\n",
    "lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [1, 1], [2, 1], [3, 1], [4, 19]]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_store = []\n",
    "for idx , i in enumerate(lists) :\n",
    "    if len(i) == 1 :\n",
    "        index_store.append([i[0],1])\n",
    "    else :\n",
    "        index_store.append([i[0],len(i) ] ) \n",
    "index_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 9, 5, 21, 16, 6, 4, 15, 8, 20],\n",
       " [3, 6, 18, 9, 10, 14, 20, 16, 15, 12],\n",
       " [16, 2, 18, 9, 12, 6, 20, 10, 17, 4],\n",
       " [16, 11, 13, 6, 17, 0, 21, 1, 19, 8],\n",
       " [17, 1, 20, 21, 19, 10, 2, 13, 18, 0],\n",
       " [7, 0, 18, 5, 21, 22, 14, 20, 17, 6],\n",
       " [6, 9, 16, 15, 8, 18, 17, 13, 5, 19]]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_mish(x) :\n",
    "    return x * tf.nn.tanh(tf.nn.softplus(x))\n",
    "activate_candidate = \\\n",
    "[tf.nn.selu, tf_mish , tf.nn.elu , tf.nn.relu , tf.nn.swish]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerForEnsemble(object) :\n",
    "    def __init__(self , fac_var,category_info) : \n",
    "        self.fac_var = fac_var\n",
    "        self.info = category_info\n",
    "    def run(self, X , select_var, name, type=\"embedding\", emb_dim = 4) :\n",
    "        inputs = []\n",
    "        for idx in select_var :\n",
    "            split = tf.slice(X, [0, idx], [-1, 1])\n",
    "            if idx in list(self.info.keys()) :\n",
    "                split =tf.cast(split, dtype=tf.int32)\n",
    "                split = tf.reshape(split,(-1,))\n",
    "                size = max(list(self.info[idx].values())) + 1\n",
    "                if type == \"embedding\" :    \n",
    "                    Cat = tf.keras.layers.Embedding(size,emb_dim)(split)\n",
    "                elif type == \"onehot\" :\n",
    "                    Cat = tf.one_hot(split ,depth=size)\n",
    "                else :\n",
    "                    raise Exception(f\"No Valid Type : {type}, Please Change the type to onehot or embedding\")\n",
    "                inputs.append(Cat)\n",
    "            else :\n",
    "                inputs.append(split)\n",
    "        x_input = tf.concat(inputs , axis = 1, name=name)\n",
    "        return x_input\n",
    "def Layer(X , hdims, name , activation,\n",
    "          batch_prob,DropoutRate,reuse = False) :\n",
    "    with tf.variable_scope(f\"EnsLayer_{name}\",reuse=reuse):\n",
    "        for idx2 , h_dim in enumerate(hdims) :\n",
    "            if idx2 == 0 :\n",
    "                W = tf.get_variable(f\"w_{idx2}\",shape=[X.get_shape()[1],h_dim])\n",
    "                B = tf.get_variable(f\"b_{idx2}\",shape=[h_dim])\n",
    "                layer = tf.matmul(X, W) + B\n",
    "            else :\n",
    "                W = tf.get_variable(f\"w_{idx2}\",shape=[hdims[idx2-1] ,h_dim ])\n",
    "                B = tf.get_variable(f\"b_{idx2}\",shape=[h_dim])    \n",
    "                layer = tf.matmul(layer, W) + B\n",
    "            if len(hdims) == idx2 + 1 :\n",
    "                logit = layer\n",
    "            else :\n",
    "                layer = tf.layers.\\\n",
    "                batch_normalization(layer, center=True, \n",
    "                                    scale=True, training=batch_prob)\n",
    "                layer = activation(layer)\n",
    "                layer = tf.nn.dropout(layer, keep_prob=DropoutRate)\n",
    "    return logit\n",
    "    \n",
    "def EnsembleNN(X , hidden = [[],[]], batch_prob=None,\n",
    "               DropoutRate=None, Combs=None) :\n",
    "    Ensembles = []\n",
    "    RepLayer = LayerForEnsemble(fac_var , cat_info)\n",
    "    with tf.variable_scope(\"NNEnsemble\") :\n",
    "        for idx , Comb in enumerate(Combs) :\n",
    "            x_input = RepLayer.run(X, Comb, f\"nnTree{idx}\",\"onehot\")\n",
    "            X_DIM = x_input.get_shape().as_list()[1]\n",
    "            dims = hidden[idx]\n",
    "            dims = [X_DIM] + dims\n",
    "            print(f\"No.{idx} nnTree : {str(dims)}\")\n",
    "            SELECT = np.random.randint(0 , len(activate_candidate) , 1)[0]\n",
    "            activation = activate_candidate[SELECT]\n",
    "            LAYER = Layer(x_input , dims, idx , activation,batch_prob,DropoutRate)\n",
    "            Ensembles.append(LAYER)\n",
    "    return Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 23\n"
     ]
    }
   ],
   "source": [
    "row , dim = Train_X_np.shape\n",
    "print(row,dim)\n",
    "target_n = 2 \n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape = [ None , dim])\n",
    "y = tf.placeholder(tf.float32, shape = [ None , 1])\n",
    "DropoutRate = tf.placeholder(tf.float32, name =\"dropoutRate\")\n",
    "training = tf.placeholder_with_default(False, shape=[], name=\"training\")\n",
    "global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n",
    "batch_prob  = tf.placeholder(tf.bool)\n",
    "batch_size = tf.placeholder(tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tuple = (X,y)\n",
    "class_dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n",
    "class_dataset = class_dataset.shuffle(buffer_size= 30000,)\n",
    "class_dataset = class_dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "iter = class_dataset.make_initializable_iterator()\n",
    "feature_x , label_y = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(select_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.0 nnTree : [10, 40, 40, 15, 1]\n",
      "No.1 nnTree : [15, 30, 10, 1]\n",
      "No.2 nnTree : [18, 40, 30, 15, 1]\n",
      "No.3 nnTree : [13, 30, 30, 15, 1]\n",
      "No.4 nnTree : [21, 30, 25, 16, 1]\n",
      "No.5 nnTree : [10, 30, 15, 1]\n",
      "No.6 nnTree : [10, 20, 10, 1]\n"
     ]
    }
   ],
   "source": [
    "HIDDEN = [\n",
    "    [ 40 , 40, 15, 1] , \n",
    "    [ 30 , 10, 1] ,\n",
    "    [ 40 , 30, 15, 1] ,\n",
    "    [ 30 , 30, 15, 1] ,\n",
    "    [ 30 , 25, 16, 1] ,\n",
    "    [ 30, 15, 1] , \n",
    "    [ 20 , 10, 1]\n",
    "]\n",
    "\n",
    "NModels = EnsembleNN(feature_x , hidden = HIDDEN , \n",
    "                     batch_prob=batch_prob, DropoutRate=DropoutRate,\n",
    "                     Combs= select_var )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_control = tf.get_variable(\"weight_control__\",shape = [len(NModels)] , \n",
    "                                  dtype = tf.float32 , \n",
    "                                  initializer = tf.constant_initializer(1/len(NModels)),\n",
    "                                 constraint= lambda x: tf.clip_by_value(x, 0, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_control = tf.nn.softmax(weights_control)\n",
    "# weights_tree_trans = tf.reshape(weights_control,(len(NModels),1,1))\n",
    "#weighted_logit = weights_control * tf.concat(NModels,axis=1)\n",
    "Probs = []\n",
    "Losses = []\n",
    "argMax = [ ]\n",
    "for idx , Model in enumerate(NModels) :\n",
    "    loss2 = tf.nn.\\\n",
    "    weighted_cross_entropy_with_logits(\n",
    "        labels = label_y , logits=Model,pos_weight=1.5)\n",
    "    PROB = tf.nn.sigmoid(Model)\n",
    "    argmax = tf.where(PROB > tf.constant(0.5), \n",
    "                      tf.ones_like(PROB), tf.zeros_like(PROB))\n",
    "    argMax.append(argmax)\n",
    "    Probs.append(PROB)\n",
    "    Losses.append(loss2)\n",
    "#     Probs += tf.nn.sigmoid(Model)\n",
    "prob = tf.reshape(tf.reduce_sum(tf.concat(Probs,axis=1)*weights_control,axis=1),(-1,1))\n",
    "total_argmax = tf.where(prob > tf.constant(0.5), \n",
    "                      tf.ones_like(prob), tf.zeros_like(prob))\n",
    "argMax.append(total_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "argMax.append(label_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_pred = tf.equal( total_argmax, label_y )\n",
    "accuracy = tf.reduce_mean(tf.cast(comp_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = tf.reduce_mean(tf.reduce_sum(tf.concat(Losses,axis=1)*weights_control,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit = tf.concat([Logit], axis = 0) * weights_tree_trans\n",
    "# Logits = tf.reduce_sum(Logit, axis = 0)\n",
    "# Probs = Probs / len(NModels)\n",
    "# weighted_mean_logit = tf.reshape(tf.reduce_mean(weighted_logit,axis=1),(-1,1))\n",
    "# prob= tf.nn.sigmoid(weighted_mean_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "auc_value , update_auc = tf.metrics.auc(label_y , prob , curve=\"ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"NNEnsemble\") \n",
    "totalvars = tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.train.cosine_decay_restarts(1e-4, global_step,\n",
    "                                               first_decay_steps=100, t_mul=1.5,m_mul=0.9, alpha=0.0)\n",
    "\n",
    "L2= []\n",
    "for v in totalvars :\n",
    "    L2.append(tf.nn.l2_loss(v))\n",
    "L2Regularizer= tf.add_n(L2)  * 0.01\n",
    "loss2 +=L2Regularizer\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    solver2 = tf.train.AdamOptimizer(learning_rate= learning_rate).minimize(loss2 ,var_list = totalvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.axes_grid1.inset_locator import (inset_axes, TransformedBbox,\n",
    "                                                   BboxPatch, BboxConnector)\n",
    "def my_mark_inset(parent_axes, inset_axes, loc1a=1, loc1b=1, loc2a=2, loc2b=2, **kwargs):\n",
    "    rect = TransformedBbox(inset_axes.viewLim, parent_axes.transData)\n",
    "    pp = BboxPatch(rect, fill=False, **kwargs)\n",
    "    parent_axes.add_patch(pp)\n",
    "    p1 = BboxConnector(inset_axes.bbox, rect, loc1=loc1a, loc2=loc1b, **kwargs)\n",
    "    inset_axes.add_patch(p1)\n",
    "    p1.set_clip_on(False)\n",
    "    p2 = BboxConnector(inset_axes.bbox, rect, loc1=loc2a, loc2=loc2b, **kwargs)\n",
    "    inset_axes.add_patch(p2)\n",
    "    p2.set_clip_on(False)\n",
    "    return pp, p1, p2\n",
    "def subplotting(ax , store , x , y ,cond={}, **kwargs) :\n",
    "    ax.plot(store[x],store[y],**kwargs)\n",
    "    if \"ylabel\" in cond :\n",
    "        ax.set_ylabel(cond[\"ylabel\"], fontsize= 10)\n",
    "    if \"xlabel\" in cond :\n",
    "        ax.set_xlabel(cond[\"xlabel\"], fontsize= 10)\n",
    "    if \"xlim\" in cond :\n",
    "        ax.set_xlim(cond[\"xlim\"])\n",
    "    if \"ylim\" in cond :\n",
    "        ax.set_ylim(cond[\"ylim\"])\n",
    "    if \"title\" in cond :\n",
    "        ax.set_title(cond[\"title\"], fontsize= 15)\n",
    "    ax.legend()\n",
    "    return ax\n",
    "## version3\n",
    "def vis_onlysl(store:dict, path:str,title:str) :\n",
    "    clear_output()\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    gs = GridSpec(nrows=2, ncols=2)\n",
    "    ax1 = fig.add_subplot(gs[0:1, 0])\n",
    "    ax2 = fig.add_subplot(gs[0:1, 1])\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.99, \n",
    "            top=0.9, wspace=0.3, hspace=0.2)\n",
    "    ax1.plot(store[\"epoch\"],store[\"slloss\"],label = \"SLloss\", color='c')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    select_sl= np.argmin(store[\"slloss\"])\n",
    "    ax1.vlines(store[\"epoch\"][select_sl],\n",
    "               np.min(store[\"slloss\"]),np.max(store[\"slloss\"]),\n",
    "               label='Best', color='c')\n",
    "    a =store[\"slloss\"][-1]\n",
    "    ax1.set_title(f'Loss : {a:.4f}')\n",
    "    ax1.set_ylabel('Supervised', fontsize= 15)\n",
    "    ax1.legend()\n",
    "    ax2.plot(store[\"epoch\"],store[\"auc\"],label = \"Train auc\")\n",
    "    ax2.plot(store[\"epoch\"],store[\"teauc\"],label = \"Test auc\")\n",
    "    ax2.set_ylabel('AUC', fontsize= 15)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    select= np.argmax(store[\"teauc\"])\n",
    "    msg = f\"test idx : {store['epoch'][select]}, maximum : {store['teauc'][select]*100:.2f}\"\n",
    "    ax2.set_title(msg)             \n",
    "#     ax2.set_ylim(0.7,store['auc'][select]+0.1)\n",
    "    ax2.legend()\n",
    "    check_n = 4\n",
    "    if len(store['epoch']) > check_n :\n",
    "        axins = inset_axes(ax2, \"100%\", \"100%\", \n",
    "                           bbox_to_anchor=[0.36, .3, .5, .4],\n",
    "                       bbox_transform=ax2.transAxes, borderpad=0)\n",
    "        axins.plot(store['epoch'], store['auc'])\n",
    "        axins.plot(store['epoch'], store['teauc'])\n",
    "        maximum = np.max(store['auc'][-check_n:] + store['teauc'][-check_n:])\n",
    "        minumum = np.min(store['auc'][-check_n:] + store['teauc'][-check_n:])\n",
    "        xlims = (store['epoch'][-check_n],store['epoch'][-1])\n",
    "        ylims = (minumum, maximum)\n",
    "        axins.set(xlim=xlims, ylim=ylims)\n",
    "        my_mark_inset(ax2, axins, loc1a=2, loc1b=3, loc2a=4, loc2b=4, fc=\"none\", ec=\"0.5\") # \n",
    "#     msg = f\"Epoch : {epoch[-1]}, Loss : {loss[-1]:.3f}, Auc : {aucs[-1]:.3f}\"\n",
    "#     skplt.metrics.plot_ks_statistic(store[\"train_y\"], store[\"train_prob\"], \n",
    "#                                 ax = ax3 ,\n",
    "#                                 title = \"[Train] KS Static PLOT\")\n",
    "#     skplt.metrics.plot_ks_statistic(store[\"test_y\"], store[\"test_prob\"], \n",
    "#                                 ax = ax4 ,\n",
    "#                                 title = \"[Test] KS Static PLOT\")\n",
    "    sns.boxplot(x=\"t\", y=\"prob\", data=store[\"train_pd\"] , ax = ax3)\n",
    "    ax3.set_title(\"train\" , fontsize= 15)\n",
    "    sns.boxplot(x=\"t\", y=\"prob\", data=store[\"test_pd\"] , ax = ax4)\n",
    "    ax4.set_title(\"test\" , fontsize= 15)\n",
    "    plt.suptitle(title)\n",
    "    plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "Epoch = 100000\n",
    "oversample = list(np.arange(len(Train_y))) + 2 * list(np.where(Train_y.values == 1)[0])\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.metrics import auc , roc_auc_score\n",
    "with tf.Session(config=config) as sess:\n",
    "    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init)\n",
    "    store = {\"epoch\" : [] ,  \"slloss\" : [],\n",
    "             \"auc\" : [], \"teauc\" : []}\n",
    "    store[\"train_pd\"] = []\n",
    "    store[\"test_pd\"] = []\n",
    "    for i in range(Epoch):    \n",
    "        batchSlLoss = []\n",
    "        permutation_idx = np.random.permutation(oversample)\n",
    "        sess.run(iter.initializer , \n",
    "                 feed_dict= {X : Train_X_np[permutation_idx],\n",
    "                             y : Train_y.values.reshape(-1,1)[[permutation_idx]],\n",
    "                             batch_size : 300,\n",
    "                            }) # switch to train dataset\n",
    "        while True :\n",
    "            try :\n",
    "                ### Supervised Learning\n",
    "                result  = sess.run([solver2 , loss2],\n",
    "                                   feed_dict={global_step:i, \n",
    "                                              batch_prob :True,\n",
    "                                              DropoutRate : 0.8\n",
    "                                             })\n",
    "                batchSlLoss.append(result[1])\n",
    "            except tf.errors.OutOfRangeError :\n",
    "                break\n",
    "        mslloss = np.mean(batchSlLoss)\n",
    "        print(f\"Epoch : {i}, SLLoss : {mslloss:.6f}\" , end = \"\\r\")\n",
    "        if (i % 100 == 0) :\n",
    "            sess.run(iter.initializer , \n",
    "                     feed_dict= {X : Train_X_np,\n",
    "                     y : Train_y.values.reshape(-1,1),\n",
    "                     batch_size : len(Train_X_np),}) \n",
    "            tr_result  = sess.run([prob,label_y],\n",
    "                                  feed_dict={batch_prob :False,\n",
    "                                             DropoutRate : 1.0\n",
    "                                            })\n",
    "            sess.run(iter.initializer , \n",
    "                     feed_dict= {X : Test_X_np,\n",
    "                     y : Test_y.values.reshape(-1,1),\n",
    "                     batch_size : len(Test_X_np),}) \n",
    "            result  = sess.run([prob,label_y,weights_control],\n",
    "                               feed_dict={batch_prob :False,\n",
    "                                          DropoutRate : 1.0\n",
    "                                         })\n",
    "            plt.scatter(np.arange(0,len(result[2])), result[2])\n",
    "            plt.savefig(f\"{weightResultPath}/plot.{i:05d}.png\")\n",
    "            a = \"Weight nnTree\" +\" \\n\"\n",
    "            b = a + str(result[2])\n",
    "            plt.title(b)\n",
    "            plt.close()\n",
    "            sess.run(iter.initializer , \n",
    "                     feed_dict= {X : Test_X_np,\n",
    "                     y : Test_y.values.reshape(-1,1),\n",
    "                     batch_size : len(Test_X_np),}) \n",
    "            dd  = sess.run(argMax+[accuracy],\n",
    "                               feed_dict={batch_prob :False,\n",
    "                                          DropoutRate : 1.0\n",
    "                                         })\n",
    "            s = pd.DataFrame(np.concatenate(dd[:-1],axis=1)).T\n",
    "            models = np.arange(s.shape[0]-2).tolist()\n",
    "            models = models + [\"pred\",\"real\"]\n",
    "            indexs = np.arange(s.shape[1])\n",
    "            fig, ax = plt.subplots( figsize = (12,8))\n",
    "            plt.pcolor(s)\n",
    "            plt.yticks(np.arange(0.5, len(models), 1),models)\n",
    "            plt.title(f\"Epoch {i} , Auccray : {dd[-1]*100:.2f}\",fontsize=20)\n",
    "            plt.savefig(f\"{testPath}/plot.{i:05d}.png\")\n",
    "            plt.close()  \n",
    "            store[\"epoch\"].append(i)\n",
    "            store[\"slloss\"].append(mslloss)\n",
    "            store[\"train_pd\"] = pd.DataFrame(np.concatenate(tr_result,axis=1),\n",
    "                                             columns=[\"prob\",\"t\"])\n",
    "            store[\"test_pd\"] = pd.DataFrame(np.concatenate(result[0:2],axis=1),\n",
    "                                             columns=[\"prob\",\"t\"])\n",
    "            store[\"auc\"].append(roc_auc_score(store[\"train_pd\"][\"t\"].values,\n",
    "                                              store[\"train_pd\"][\"prob\"].values))\n",
    "            store[\"teauc\"].append(roc_auc_score(store[\"test_pd\"][\"t\"].values,\n",
    "                                                store[\"test_pd\"][\"prob\"].values))\n",
    "#             store[\"train_prob\"] =  np.concatenate((1-tr_result[2],tr_result[2]),axis=1)\n",
    "#             store[\"test_prob\"] =  np.concatenate((1-result[2],result[2]),axis=1)\n",
    "            vis_onlysl(store, f\"{ResultPath}/0405_SL_EnsembelNN.png\",\n",
    "                       title=\"Train Ensemblem Neural Network\")\n",
    "            clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dd[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in  category_info.items() :\n",
    "    a[key] = pd.Categorical(a[key],list(value.values()))\n",
    "pd.get_dummies(a,columns=fac_var).columnsumns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEX_1</th>\n",
       "      <th>SEX_2</th>\n",
       "      <th>SEX_3</th>\n",
       "      <th>EDUCATION_1</th>\n",
       "      <th>EDUCATION_2</th>\n",
       "      <th>EDUCATION_3</th>\n",
       "      <th>EDUCATION_4</th>\n",
       "      <th>EDUCATION_5</th>\n",
       "      <th>EDUCATION_6</th>\n",
       "      <th>EDUCATION_7</th>\n",
       "      <th>EDUCATION_8</th>\n",
       "      <th>MARRIAGE_1</th>\n",
       "      <th>MARRIAGE_2</th>\n",
       "      <th>MARRIAGE_3</th>\n",
       "      <th>MARRIAGE_4</th>\n",
       "      <th>MARRIAGE_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SEX_1, SEX_2, SEX_3, EDUCATION_1, EDUCATION_2, EDUCATION_3, EDUCATION_4, EDUCATION_5, EDUCATION_6, EDUCATION_7, EDUCATION_8, MARRIAGE_1, MARRIAGE_2, MARRIAGE_3, MARRIAGE_4, MARRIAGE_5]\n",
       "Index: []"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
